{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Students\n",
                "- Student 1: <span style=\"color:green\">253885 - Luca Franceschi</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 5: Sructure from motion\n",
                "\n",
                "## Goals\n",
                "The goal of the current assignment is to learn the following concepts:\n",
                "\n",
                "- How to self-calibrate a camera using vanishing points.\n",
                "- What are the main elements of an incremental structure from motion approach.\n",
                "\n",
                "## Introduction\n",
                "\n",
                "The Structure fron Motion problem (SfM) is defined as the 3D reconstruction from a set of unordered and uncalibrated images. There are different SfM approaches: global, hierarchical or incremental. In this lab we will focus on the incremental SfM, which is the most popular strategy for 3D reconstruction from unordered and uncalibrated photo collections.\n",
                "\n",
                "Incremental SfM is a sequential processing pipeline with an iterative component. It starts working from two views, from which motion (camera parameters) are initialized, and then the structure (3D reconstruction) is also initialized. Right after, an iterative extension of both the motion and the structure is performed, progressively adding new cameras/views and 3D points. A further refinement of both the camera matrices and the reconstructed point cloud is carried out. This last step is known as Bundle Adjustment and it involves solving an optimization problem by iterative techniques.\n",
                "\n",
                "The purpose of this lab is to familiarize with the structure from motion problem and work with the main blocks that constitute a basic (vanilla) incremental SfM algorithm, without the refinement step. A more complete and robust SfM pipeline would include -- apart from the bundle adjustment --  a carefully selection of the two initial views and each next new view to incorporate, filtering of outliers,  and some other tricks [3]. If you are interested in a more complete solution here we provide a list of some of the most known libraries and softwares:\n",
                "\n",
                "- OpenMVG:  http://openmvg.readthedocs.io/en/latest/# <br>\n",
                "Incremental and global SfM, open source.\n",
                "\n",
                "- VisualSFM:  http://ccwu.me/vsfm/ <br>\n",
                "Incremental SfM, very efficient, GUI, binaries.\n",
                "\n",
                "- Bundler: http://www.cs.cornell.edu/~snavely/bundler/ <br>\n",
                "Incremental SfM, open source.\n",
                "\n",
                "- Colmap: http://colmap.github.io/ <br>\n",
                "Incremental SfM, very efficient, nice GUI, open source.\n",
                "\n",
                "- Theia: http://www.theia-sfm.org/ <br>\n",
                "Incremental and Global SfM, very efficient, open source.\n",
                "\n",
                "The solution of the structure from motion strongly relies on point correspondences (matchings) across the different views, commonly known as feature tracks. Then, one of the first things to do is feature extraction and matching, followed by geometric verification to remove outliers. \n",
                "\n",
                "As in previous labs, we will be using SIFT for estimating keypoints and matchings between pairs of images.  These matchings will contain outliers; these can be filtered by robustly estimating a fundamental matrix. Moreover, the fundamental matrix that relates the two initial views will be used to estimate the camera parameters (motion) of these two views.\n",
                "\n",
                "You will have to answer the questions and complete the provided code when necessary as required. **You must deliver the completed (and executed) ipynb file, including the answers to the questions (please make clear visually what it is answer, either preceding it by ANSWER and/or changing its color).**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import math\n",
                "import random\n",
                "import sys\n",
                "\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import plotly.graph_objects as go\n",
                "import scipy.io as sio\n",
                "import seaborn_image as isns\n",
                "from tqdm.notebook import tqdm\n",
                "from utils import *"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will work with 3 differrent views of a scene from the UPF campus."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read images\n",
                "img1_color = cv2.imread(f\"images/v1.jpg\", cv2.IMREAD_COLOR)\n",
                "img2_color = cv2.imread(f\"images/v2.jpg\", cv2.IMREAD_COLOR)\n",
                "img3_color = cv2.imread(f\"images/v3.jpg\", cv2.IMREAD_COLOR)\n",
                "\n",
                "img1_color = cv2.cvtColor(img1_color, cv2.COLOR_BGR2RGB)\n",
                "img2_color = cv2.cvtColor(img2_color, cv2.COLOR_BGR2RGB)\n",
                "img3_color = cv2.cvtColor(img3_color, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "# Reduce image size to speed up computations\n",
                "original_shape = np.array(img1_color.shape[:2])\n",
                "scale_percent = 50\n",
                "rescaled_shape = np.flip(original_shape * scale_percent // 100)\n",
                "img1r = cv2.resize(img1_color, rescaled_shape)\n",
                "img2r = cv2.resize(img2_color, rescaled_shape)\n",
                "img3r = cv2.resize(img3_color, rescaled_shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "g = isns.ImageGrid([img1_color, img2_color, img3_color], height=5, cmap=\"gray\")\n",
                "for i, axis in enumerate(g.axes[0], start=1):\n",
                "    axis.set_title(f\"View {i}\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q1.** Robustly estimate the fundamental matrix that relates views 1 and 2, and views 1 and 3. For each case, display the inlier matchings together with the pair of images.\n",
                "\n",
                "We recommend you to use SIFT with 13000 matches instead of 3000 which is the value in previous labs.\n",
                "\n",
                "**Hint**: For a faster convergence of the RANSAC algorithm you can filter matched keypoints based on their matching distance. You can filter out matches that have a distance bigger than a threshold (e.g. 200).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find keypoints\n",
                "th = 3\n",
                "keypoints = [None]*3\n",
                "\n",
                "# Initiating SIFT detector\n",
                "sift = cv2.SIFT_create(13000)\n",
                "\n",
                "# Finding the keypoints and descriptors\n",
                "keypoints[0], des1 = sift.detectAndCompute(img1r, None)\n",
                "keypoints[1], des2 = sift.detectAndCompute(img2r, None)\n",
                "keypoints[2], des3 = sift.detectAndCompute(img3r, None)\n",
                "\n",
                "# Keypoint matching\n",
                "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
                "matches_12 = bf.match(des1, des2)\n",
                "matches_13 = bf.match(des1, des3)\n",
                "\n",
                "# Show matches\n",
                "def plot_matches(img1, img2, kp1, kp2, matches, title=\"Matches\"):\n",
                "    \"\"\"\n",
                "    Plot matches between two images.\n",
                "    \"\"\"\n",
                "    img_matches = cv2.drawMatches(\n",
                "        img1,\n",
                "        kp1,\n",
                "        img2,\n",
                "        kp2,\n",
                "        matches,\n",
                "        None,\n",
                "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
                "    )\n",
                "    plt.figure(figsize=(20, 10))\n",
                "    plt.imshow(img_matches)\n",
                "    plt.axis(\"off\")\n",
                "    plt.title(title)\n",
                "    plt.show()\n",
                "plot_matches(img1r, img2r, keypoints[0], keypoints[1], matches_12, \"Matches 1-2\")\n",
                "plot_matches(img1r, img3r, keypoints[0], keypoints[2], matches_13, \"Matches 1-3\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Filter matches based on hint\n",
                "old_matches_12 = matches_12\n",
                "old_matches_13 = matches_13\n",
                "\n",
                "matches_12 = np.array(matches_12)[[match.distance < 200 for match in matches_12]].tolist()\n",
                "matches_13 = np.array(matches_13)[[match.distance < 200 for match in matches_13]].tolist()\n",
                "\n",
                "print(f\"Images 1-2 - {len(matches_12)/len(old_matches_12)*100:.2f}% of matches after filtering: {len(matches_12)} of {len(old_matches_12)}\")\n",
                "print(f\"Images 1-3 - {len(matches_13)/len(old_matches_13)*100:.2f}% of matches after filtering: {len(matches_13)} of {len(old_matches_13)}\")\n",
                "\n",
                "# Show filtered matches\n",
                "plot_matches(img1r, img2r, keypoints[0], keypoints[1], matches_12, \"Filtered Matches 1-2\")\n",
                "plot_matches(img1r, img3r, keypoints[0], keypoints[2], matches_13, \"Filtered Matches 1-3\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find the fundamental matrices and inlier matches for views 1-2 and 1-3\n",
                "\n",
                "points1_H12, points2_H12 = correspondences_between_keypoints(keypoints[0], keypoints[1], matches_12)\n",
                "points1_H13, points3_H13 = correspondences_between_keypoints(keypoints[0], keypoints[2], matches_13)\n",
                "\n",
                "F_12, inliers_12 = ransac_fundamental_matrix(points1_H12, points2_H12, th, 1000)\n",
                "F_13, inliers_13 = ransac_fundamental_matrix(points1_H13, points3_H13, th, 1000)\n",
                "\n",
                "matches_12 = np.array(matches_12)[inliers_12].tolist()\n",
                "matches_13 = np.array(matches_13)[inliers_13].tolist()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show inlier matches for views 1-2 and 1-3\n",
                "# NOTE: make sure that matches is an array of cv2.DMatch objects\n",
                "plot_matches(img1r, img2r, keypoints[0], keypoints[1], matches_12, \"Inlier Matches for F 1-2\")\n",
                "plot_matches(img1r, img3r, keypoints[0], keypoints[2], matches_13, \"Inlier Matches for F 1-3\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question 1:</strong>\n",
                "  <ul style=\"margin: 10px 0 0 20px; padding: 0;\">\n",
                "    <li>Explain the procedure followed above.</li>\n",
                "    <li>How did you find keypoints?</li>\n",
                "    <li>How did you match them?</li>\n",
                "    <li>How did you filter them?</li>\n",
                "    <li>How did you find the Fundamental matrix?</li>\n",
                "    <li>Which distance was applied to determine which matches were inliers? Does it measure the distance between two points or the distance between a point and a line?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initial two-view reconstruction and self-calibration\n",
                "In lab 4 we saw how to recover the motion between a pair of calibrated cameras. In SfM, cameras are not calibrated, but different self-calibration techniques can be applied for estimating the intrinsic parameters.\n",
                " \n",
                "In case we work with images of man-made environments, like urban or indoor scenes, it is possible to estimate vanishing points of orthogonal directions. Vanishing points are useful for self-calibration because they allow us to establish constraints on the internal parameters of the camera (intrinsics). There are different methods in the literature to estimate vanishing points. For example, the following ones are implemented in Matlab:\n",
                "\n",
                "- Vanishing Point Detection in Urban Scenes Using Point Alignments [1, 2]: <br>\n",
                "http://www.ipol.im/pub/art/2017/148/\n",
                "\n",
                "- Orthogonal Vanishing Points in Uncalibrated Images of Man-Made Environments [4]: <br>\n",
                "https://members.loria.fr/GSimon/software/fastvp/\n",
                "\n",
                "And this one in Python:\n",
                "- NeurVPS: Neural Vanishing Point Scanning via Conic Convolution: [6] <br>\n",
                "https://github.com/zhou13/neurvps\n",
                "\n",
                "We will assume square pixels (aspect ratio of 1), zero skew factor and principal point at the center of the image, which is common in most commercial cameras.  Then, the only remaining unknown in the matrix of internal parameters is the focal length $\\alpha$, but it can be estimated using a pair of vanishing points as we will explain now (see also Section 6.3.2 of Szeliski's book [5]). \n",
                "\n",
                "Under the previous assumptions, the camera calibration matrix, $K$, can be written as: <br>\n",
                "\n",
                "$$ K=\\begin{pmatrix} \\alpha _x & s & c_x \\\\ 0 & \\alpha _y & c_y \\\\ 0 & 0 & 1\\end{pmatrix}\n",
                "\\overset{Assumptions}{\\longrightarrow}\n",
                "K=\\begin{pmatrix} \\alpha & 0 & c_x \\\\ 0 & \\alpha & c_y \\\\ 0 & 0 & 1\\end{pmatrix}$$\n",
                "where the only unknown is $\\alpha$, since $c_x = \\frac{w}{2}$ and $c_y = \\frac{h}{2}$, being $w$ and $h$, respectively, the image width and height in pixels. \n",
                "\n",
                "Let us assume that we have detected two or more orthogonal vanishing points, all of which are finite, i.e., they are not obtained from lines that appear to be parallel in the image plane. The projection equation for the vanishing point $\\mathbf{v}_1$ corresponding to the cardinal direction $(1, 0, 0)^T$ can be written as\n",
                "$$ \\mathbf{v}_1 \\sim P \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = K  \\begin{pmatrix}\\mathbf{r}_1 & \\mathbf{r}_2 & \\mathbf{r}_3 & \\mathbf{t} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = K \\mathbf{r}_1$$\n",
                "If we denote the coordinates of the vanishing point $\\mathbf{v}_1$ as $x_1$ and $y_1$ we have:\n",
                "$$ \\mathbf{r}_1 \\sim K^{-1} \\mathbf{v}_1 = \\begin{pmatrix} 1/\\alpha & 0 & -c_x/\\alpha \\\\ 0 & 1/\\alpha & -c_y/\\alpha \\\\ 0 & 0 & 1 \\end{pmatrix} \n",
                " \\begin{pmatrix} x_1 \\\\ y_1 \\\\ 1 \\end{pmatrix} = \n",
                " \\begin{pmatrix} (x_1 - c_x) / \\alpha \\\\ (y_1 -c_y) / \\alpha \\\\ 1 \\end{pmatrix}\n",
                " \\sim \\begin{pmatrix} x_1 - c_x \\\\ y_1 -c_y \\\\ \\alpha \\end{pmatrix}$$\n",
                "And in general, for the vanishing point $\\mathbf{v}_i$, $i=1,2,3$, corresponding to one of the cardinal directions (1, 0, 0), (0, 1, 0), or (0, 0, 1) respectively, and $\\mathbf{r}_i$ being the $i_{th}$ column of the rotation matrix $R$ we have:\n",
                "$$ \\mathbf{r}_i \\sim  \\begin{pmatrix} x_i - c_x \\\\ y_i -c_y \\\\ \\alpha \\end{pmatrix}$$\n",
                "From the orthogonality between columns of the rotation matrix, we have:\n",
                "$$ \\mathbf{r}_i  \\cdot \\mathbf{r}_j = (x_i - c_x)(x_j - c_x)+(y_i - c_y)(y_j - c_y)+ \\alpha^2 =0, $$\n",
                "from which we can obtain an estimate for $\\alpha$:\n",
                "$$ \\alpha = \\sqrt{-(x_i - c_x)(x_j - c_x)-(y_i - c_y)(y_j - c_y)}.$$\n",
                "Then, it is possible to estimate $\\alpha$, and thus $K$, using two vanishing points corresponding to orthogonal directions. In our case, all the images have been taken with the same camera, so all of them will share the same $K$.\n",
                "\n",
                "**Q2.** Provide the code to estimate the matrix of internal parameters following the previous directions. Which is the matrix you have obtained?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vanishing points are provided below\n",
                "vp1 = np.array([1932.51919443, 3033.59044871])\n",
                "vp2 = np.array([1516.57688111, -14998.62215829])\n",
                "vp1, vp2 = vp1 * scale_percent / 100, vp2 * scale_percent / 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Complete\n",
                "x1, y1 = vp1\n",
                "x2, y2 = vp2\n",
                "ny, nx, _ = img1r.shape # NOTE: remember (height, width)\n",
                "cy = ny / 2 # REMEMBER HALF !! NOT FULL IMAGE SIZES\n",
                "cx = nx / 2\n",
                "\n",
                "alpha = np.sqrt(-(x1-cx)*(x2-cx)-(y1-cy)*(y2-cy))\n",
                "K = np.array([[alpha, 0,     cx],\n",
                "              [0,     alpha, cy],\n",
                "              [0,     0,     1]])\n",
                "\n",
                "print(f\"alpha: {alpha}\")\n",
                "print(f\"Intrinsic matrix K:\\n{K}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Optional question 2**: \n",
                "\n",
                "Given what we have learned in slides 29-32 from lecture 5, we can find a different way to estimate $\\alpha$. We know that $v_1^T \\omega v_2 = 0$. We should explain that with the image of the absolute conic we can obtain the formula for the angle between 2 vectors. $cos(\\theta) = \\frac{v_1^T \\omega v_2}{...}$, and knowing that $v_1$ and $v_2$ are orthogonal, we know that the numerator is equal to zero. Then, we know that $\\omega = (K K^T)^{-1}$.\n",
                "\n",
                "\n",
                "Let's understand the shape of $\\omega$. We know that in the general case\n",
                "$$ K = \\begin{pmatrix} \\alpha_x & s & c_x \\\\ 0 & \\alpha_y & c_y \\\\ 0 & 0 & 1\\end{pmatrix} $$\n",
                "Then, the value of $\\omega$ can be computed and the result is the following one (extracted from Zhang paper provided in lab 2):\n",
                "\n",
                "$$\\omega = \\begin{pmatrix}\n",
                "\\frac{1}{\\alpha_x^2} & - \\frac{s}{\\alpha_x^2\\alpha_y} & \\frac{c_y s - c_x \\alpha_y}{\\alpha_x^2\\alpha_y} \\\\\n",
                "- \\frac{s}{\\alpha_x^2\\alpha_y} & \\frac{s^2}{\\alpha_x^2\\alpha_y} + \\frac{1}{\\alpha_y^2} & - \\frac{s (c_y s - c_x \\alpha_y)}{\\alpha_x^2\\alpha_y} - \\frac{c_y}{\\alpha_y^2} \\\\\n",
                "\\frac{c_y s - c_x \\alpha_y}{\\alpha_x^2\\alpha_y} & - \\frac{s (c_y s - c_x \\alpha_y)}{\\alpha_x^2\\alpha_y} - \\frac{c_y}{\\alpha_y^2} & \\frac{(c_y s - c_x \\alpha_y)^2}{\\alpha_x^2\\alpha_y^2} + \\frac{c_y^2}{\\alpha_y^2} + 1\\end{pmatrix}\n",
                "$$\n",
                "\n",
                "Apply the assumptions of squared pixels and zero skew factor to demonstrate the same equation of $\\alpha$ as in the previous explanation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<span style=\"color:green\">\n",
                "\n",
                "OPTIONAL ANSWER: write your theoretical answer in LaTex or add a picture of your notes.\n",
                "\n",
                "</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
                "    <strong>🎥 OPTIONAL Video Question 2:</strong><br>\n",
                "    <li>Explain the theoretical answer above.<br>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "-------\n",
                "\n",
                "In the first part of the lab we have estimated the fundamental matrix $F$ that relates the initial pair of views. With $F$ and $K$ we can estimate the essential matrix $E$ and from it we can get the complete camera matrices (intrinsics and extrinsics) for the initial pair of views. Once the cameras are fully calibrated an initial 3D reconstruction (structure) is found by triangulation. These steps were part of lab 3.\n",
                "\n",
                "\n",
                "**Q3.** Estimate the camera matrices for views 1 and 2. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find P1\n",
                "P1 = K @ np.eye(3, 4)\n",
                "print(f\"P1: {P1}\")\n",
                "\n",
                "# TODO: Find Essential matrix for view 1-2\n",
                "E_12 = K.T @ F_12 @ K\n",
                "print(f\"Essential matrix E_12:\\n{E_12}\")\n",
                "\n",
                "# TODO: Find P2\n",
                "ny, nx, _ = img1r.shape\n",
                "P2 = camera_projection_matrix(F_12, K, nx, ny, points1_H12, points2_H12, inliers_12)\n",
                "print(f\"P2: {P2}\")\n",
                "\n",
                "# Show the projection matrices\n",
                "ny, nx, _ = img1r.shape\n",
                "fig = go.Figure()\n",
                "plot_camera(P1, nx, ny, fig, \"Camera 1\")\n",
                "plot_camera(P2, nx, ny, fig, \"Camera 2\")\n",
                "fig.update_layout(\n",
                "    title=\"Camera Projection Matrices\",\n",
                "    scene=dict(\n",
                "        xaxis_title=\"X\",\n",
                "        yaxis_title=\"Y\",\n",
                "        zaxis_title=\"Z\",\n",
                "        aspectmode=\"data\"\n",
                "    ),\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
                "    <strong>🎥 Video Question (Q3):</strong><br>\n",
                "    <li>Explain the procedure followed to find P1, E_12, and P2. Explain it step by step referring to the code above.<br>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q4.** Triangulate the matches from views 1 and 2 and plot them together with the cameras."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# complete ...\n",
                "# TODO: Triangulate all matches\n",
                "X = ...\n",
                "\n",
                "print(f\"Number of points in the point cloud: {X.shape[1]}\")\n",
                "\n",
                "\n",
                "# TODO: Remove bad-triangulated points (Z < 0) for a proper visualization\n",
                "\n",
                "print(f\"Number of points in the point cloud after removing errors: {X.shape[1]}\")\n",
                "\n",
                "# Render the 3D point cloud\n",
                "fig = go.Figure()\n",
                "plot_camera(P1, nx, ny, fig, \"Camera 1\")\n",
                "plot_camera(P2, nx, ny, fig, \"Camera 2\")\n",
                "x_img = inliers_1_12[:2].T.astype(int) # inliers_1_12 are the points in the first image for inliers of view 1-2\n",
                "rgb_vals = img1r[x_img[:, 1], x_img[:, 0]]\n",
                "rgb_vals = [f\"rgb({int(r)},{int(g)},{int(b)})\" for r, g, b in rgb_vals]\n",
                "point_color = [(255, 0, 0), (0, 255, 0)]\n",
                "fig.add_trace(go.Scatter3d(x=X[0, :], y=X[2, :], z=-X[1, :], mode=\"markers\", marker=dict(size=2, color=rgb_vals)))\n",
                "fig.update_layout(\n",
                "    title=\"3D Point Cloud - View 1-2\",\n",
                "    scene=dict(\n",
                "        xaxis_title=\"X\",\n",
                "        yaxis_title=\"Y\",\n",
                "        zaxis_title=\"Z\",\n",
                "        aspectmode=\"data\"\n",
                "    ),\n",
                ")\n",
                "fig.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
                "    <strong>🎥 Video Question (Q4):</strong><br>\n",
                "    <li>Explain the results obtained. Is the result correct? Can you find objects in the 3D point cloud?<br>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Estimate new camera pose from structure\n",
                "\n",
                "At this point we have reconstructed some 3D points from the point correspondences in the initial pair of views. If we are able to find a sufficient number of correspondences, in a new image, of the already reconstructed keypoints we will have a set of 3D-2D correspondences that can be used to calibrate the new view. For that, we will use the resectioning method (lecture 6) that needs  at least six 3D-2D correspondences. Alternatively, other methods, like $PnP$ approaches can be used for this purpose (since all cameras share the same matrix $K$).\n",
                "\n",
                "**Q5.** Find the intersection matches between matches 1-2 and 1-3.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find intersection matches between views 1-2 and 1-3\n",
                "intersect_12, intersect_13 = [], []\n",
                "# complete ...\n",
                "kp1 = ...\n",
                "kp2 = ...\n",
                "kp3 = ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Print number of intersection matches and show matches on images\n",
                "print(f\"Number of intersection matches: {kp1.shape[1]} out of {len(matches_12)}\")\n",
                "plot_matches(img1r, img2r, keypoints[0], keypoints[1], intersect_12, \"Intersection Matches 1–2\")\n",
                "plot_matches(img1r, img3r, keypoints[0], keypoints[2], intersect_13, \"Intersection Matches 1–3\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
                "    <strong>🎥 Video Question (Q5):</strong><br>\n",
                "    <li>Briefly explain the purpose of the code above.<br>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q6.** Create the function `resectioning` to calibrate the 3rd view and establish the proper entries to the function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Define points_2d and points_3d\n",
                "points_2d = ...\n",
                "points_3d = ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# TODO: Define resectioning function\n",
                "def resectioning(points_2d, points_3d): ...\n",
                "\n",
                "# Find P3\n",
                "P3_resectioned, inliers_3 = resectioning(points_2d, points_3d) ## P3_resectioned, inliers_3 = resectioning(points_2d, points_3d)\n",
                "print(f\"Number of inliers for P3: {len(inliers_3)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
                "    <strong>🎥 Video Question (Q6):</strong><br>\n",
                "    <li>Explain the resectioning technique step by step.</li>\n",
                "    <li>How many correspondences are required? Why?</li><br>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You will obtain $P_3$ by resectioning. However, in order to plot $P_3$, we will have to normalize it because the current scaling factor can be of any value. To do that we will decompose the projection matrix into $K_3$, $R_3$ and $t_3$. Normalize $K_3$ with its last element, dehomogenise $t_3$, and compute the new normalised projection matrix of camera 3, `P3_norm`.   "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Decompose P3\n",
                "K_3, R_3, t_3 = cv2.decomposeProjectionMatrix(P3)[:3]\n",
                "K_3 /= ...\n",
                "t_3 = t_3[:, 0] / ...\n",
                "P3 = ??? @ np.column_stack((???, ???))\n",
                "\n",
                "print(f\"Previous K:{K}\", f\"K_3: {K_3}\", f\"R_3: {R_3}\", f\"t_3: {t_3}\", f\"P3: {P3}\", sep=\"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q7):</strong>\n",
                "</div>\n",
                "\n",
                "Try to understand the values of $K_3$, $R_3$ and $t_3$.\n",
                "- Is $K_3$ similar to the previously estimated $K$? Should it be similar?\n",
                "- What can you interpret from $R_3$? How is this rotation like? Which are the reference coordinates of this rotation?\n",
                "- What can you interpret from $t_3$? How is the translation like? Which are the reference coordinates of this translation?\n",
                "- Visualise the plot below. Are your interpretations shown in the plot?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show the projection matrices\n",
                "ny, nx, _ = img1r.shape\n",
                "fig = go.Figure()\n",
                "plot_camera(P1, nx, ny, fig, \"Camera 1\")\n",
                "plot_camera(P2, nx, ny, fig, \"Camera 2\")\n",
                "plot_camera(P3, nx, ny, fig, \"Camera 3\")\n",
                "fig.update_layout(\n",
                "    title=\"Estimated Camera Projection Matrices\",\n",
                "    scene=dict(\n",
                "        xaxis_title=\"X\",\n",
                "        yaxis_title=\"Y\",\n",
                "        zaxis_title=\"Z\",\n",
                "        aspectmode=\"data\"\n",
                "    ),\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's show the 3D point clouds with all the cameras."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ny, nx, _ = img1r.shape\n",
                "fig = go.Figure()\n",
                "plot_camera(P1, nx, ny, fig, \"Camera 1\")\n",
                "plot_camera(P2, nx, ny, fig, \"Camera 2\")\n",
                "plot_camera(P3, nx, ny, fig, \"Camera 3\")\n",
                "fig.add_trace(go.Scatter3d(x=X[0, :], y=X[2, :], z=-X[1, :], mode=\"markers\", marker=dict(size=2, color=rgb_vals)))\n",
                "fig.update_layout(\n",
                "    title=\"Estimated 3D Points and Camera Projection Matrices\",\n",
                "    scene=dict(\n",
                "        xaxis_title=\"X\",\n",
                "        yaxis_title=\"Y\",\n",
                "        zaxis_title=\"Z\",\n",
                "        aspectmode=\"data\"\n",
                "    ),\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q8):</strong>\n",
                "  <ul>\n",
                "    <li>Evaluate qualitatively the 3D representation that you have obtained.</li>\n",
                "    <li>Which elements can you recognise from the image? Is there anything poorly represented? Interpret why.</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Optional 2**: Triangulate the matches from the 1st and 3rd view in order to add new 3D points in the point cloud. You can also the function findFundamentalMat from OpenCV (in place of your own function created in previous labs).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. References"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Add here the material you used to complete this Lab. Cite and describe the usage of AI tools if any was used according to the Guidelines for AI tools."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TODO: Complete"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Questions</strong>: Briefly mention the references.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Self-Assessment and Conclusions</strong>:\n",
                "  <ul>\n",
                "  <li><b>Which parts of the notebook did you succeed in? </b><br>\n",
                "  <em>Describe the sections where you felt confident, and explain why you think they were successful.</em></li>\n",
                "  <li><b>Which parts of the notebook did you fail to solve? </b><br>\n",
                "  <em>Be honest about the areas where you faced difficulties. What challenges or issues did you encounter that you couldn’t resolve? How would you approach these issues in the future?</em></li>\n",
                "  </ul>\n",
                "  Is there anything else that you would like to comment?\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lab References\n",
                "\n",
                "[1] José Lezama, Rafael Grompone von Gioi, Gregory Randall, and Jean-Michel Morel. Finding vanishing points via point alignments in image primal and dual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 509–515, 2014.\n",
                "\n",
                "[2] José Lezama, Gregory Randall, and Rafael Grompone von Gioi. Vanishing Point Detection in Urban Scenes Using Point Alignments. Image Processing On Line, 7:131–164, 2017.\n",
                "\n",
                "[3] Johannes Lutz Schönberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n",
                "\n",
                "[4] Gilles Simon, Antoine Fond, and Marie-Odile Berger. A simple and effective method to detect orthogonal vanishing points in uncalibrated images of man-made environments. In Eurographics, 2016.\n",
                "\n",
                "[5] Richard Szeliski. Computer vision: algorithms and applications. Springer Science & Business Media, 2010.\n",
                "\n",
                "[6] Yichao Zhou, Haozhi Qi, Jingwei Huang, and Yi Ma. Neurvps: Neural vanishing point scanning via conic convolution. NeurIPS 2019. "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
