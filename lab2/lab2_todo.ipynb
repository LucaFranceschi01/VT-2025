{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 2: Camera Calibration Using a Planar Pattern\n",
                "\n",
                "Welcome to your second lab! In this assignment, youâ€™ll explore how to **calibrate a camera** using a planar pattern. This is a fundamental step in many computer vision applications, including Augmented Reality. Get ready to recover internal camera parameters and visualize your camera in 3D!\n",
                "\n",
                "## About the Lab\n",
                "### What You'll Learn\n",
                "1. **Estimating Internal Camera Parameters:**  \n",
                "   You'll learn how to compute the internal parameters of a camera from images of a planar pattern using the **Image of the Absolute Conic (IAC)**.\n",
                "\n",
                "2. **Finding Camera Pose:**  \n",
                "   From homographies, youâ€™ll extract the **pose of the camera** (its rotation and translation) relative to the calibration pattern.\n",
                "\n",
                "3. **Augmented Reality Basics:**  \n",
                "   By estimating camera pose, youâ€™ll be able to **overlay virtual 3D objects** onto the images â€“ just like in AR apps!\n",
                "\n",
                "### Assumptions\n",
                "We will assume you are already familiar with the following concepts:\n",
                "\n",
                "- The definition of a **homography**, how to match **ORB/SIFT/SURF keypoints** between images, and how to robustly compute a homography from point correspondences.\n",
                "\n",
                "- The **camera projection matrix**, including the difference between internal and external camera parameters, and their combined matrix form: $P = K ( R \\mid t)$\n",
                "\n",
                "- The concept of the **Image of the Absolute Conic (IAC)**, and its relationship to the internal calibration matrix \\( K \\), given by: $\\omega = (K K ^T)^{-1} = K^{-T} K^{-1}$\n",
                "\n",
                "### Deliverables:\n",
                "1. ðŸ’» **Complete the Code:**  \n",
                "   Youâ€™ll implement the necessary steps to calibrate the camera and visualize its pose. Submit your completed and fully-executed Jupyter Notebook (.ipynb).\n",
                "\n",
                "2. ðŸŽ¥ **Explain Your Work:**  \n",
                "   Record a short video explaining your approach and results. Follow the same format as in Lab 1 â€” walk us through your code and outcomes.\n",
                "\n",
                "Letâ€™s go calibrate some cameras! ðŸ”§ðŸ“·âœ¨\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>ðŸŽ¥ Introduction</strong>:\n",
                "  <ul>\n",
                "    <li>Provide an overall explanation of the lab. Describe the goals of the lab clearly.</li>\n",
                "    <li>State the problem you were trying to solve or explore in this lab.</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 0 Import libraries\n",
                "If the setup was correct, you should be able to import the libraries below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import plotly.graph_objects as go\n",
                "from matplotlib import pyplot as plt\n",
                "from utils import * # Ransac_DLT_homography, plot_camera, plot_image_origin\n",
                "from scipy.linalg import cholesky"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you were able to **import the libraries without errors**, you can **remove cells of the options that you are NOT using**. It will be easier to run the entire notebook."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Calibration with a planar pattern\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The Planar Pattern\n",
                "\n",
                "For calibration, we need a known planar pattern. This is typically a checkerboard (left), which allows easy detection of corners. However, for simplicity, we will instead display a **texture-rich image** (center) on a flat screen (right). This image will serve as our calibration target.\n",
                "\n",
                "To detect and match points between the calibration image on the screen and the reference image stored on your computer, youâ€™ll use the **SIFT descriptor**. These correspondences will then allow you to compute the homographies needed for calibration.\n",
                "\n",
                "<div style=\"display: flex; gap: 20px;\">\n",
                "  <div style=\"flex: 1; text-align: center;\">\n",
                "    <img src=\"./Data/Checkerboard.png\" alt=\"Checkerboard\" width=\"70%\"><br>\n",
                "    <strong>Checkerboard Pattern</strong>\n",
                "  </div>\n",
                "  <div style=\"flex: 1; text-align: center;\">\n",
                "    <img src=\"./Data/template.jpg\" alt=\"Template\" width=\"80%\"><br>\n",
                "    <strong>Calibration Template</strong>\n",
                "  </div>\n",
                "  <div style=\"flex: 1; text-align: center;\">\n",
                "    <img src=\"./Data/image1.jpg\" alt=\"Template\" width=\"80%\"><br>\n",
                "    <strong>Calibration template displayed on a flat screen</strong>\n",
                "  </div>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.0 Visualise template and images from different positions\n",
                "Let's first visualise the template image and the images containing a planar surface with the template from different positions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the template image and convert it to RGB color space\n",
                "template_path = \"Data/template.jpg\"\n",
                "template_color = cv2.imread(template_path, cv2.IMREAD_COLOR)\n",
                "template_color_rgb = cv2.cvtColor(template_color, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "# Number of images\n",
                "N = 3\n",
                "\n",
                "# Load the images and convert them to RGB color space\n",
                "image_paths = [f\"Data/image{i}.jpg\" for i in range(1, N + 1)]\n",
                "images_color_rgb = [cv2.cvtColor(cv2.imread(path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB) for path in image_paths]\n",
                "\n",
                "# Plot the template and images\n",
                "fig, axes = plt.subplots(1, N + 1, figsize=(5 * (N + 1), 5))\n",
                "fig.suptitle(\"Template and Images of the Template in Different Positions\")\n",
                "\n",
                "# Plot the template\n",
                "axes[0].imshow(template_color_rgb)\n",
                "axes[0].set_title(\"Template\")\n",
                "axes[0].axis(\"off\")\n",
                "\n",
                "# Plot the images\n",
                "for i, img in enumerate(images_color_rgb, start=1):\n",
                "    axes[i].imshow(img)\n",
                "    axes[i].set_title(f\"Image {i}\")\n",
                "    axes[i].axis(\"off\")\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Compute Homographies Between Template and Images\n",
                "As in the previous session, we will compute homographies between the template and the images using SIFT matching and RANSAC."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load template and images in grayscale\n",
                "template_path = \"Data/template.jpg\"\n",
                "template = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE)\n",
                "images = [cv2.imread(f\"Data/image{i}.jpg\", cv2.IMREAD_GRAYSCALE) for i in range(1, N + 1)]\n",
                "\n",
                "# Find keypoints and descriptors of template and images with SIFT\n",
                "sift = cv2.SIFT_create(4000)\n",
                "keypoints_template, descriptors_template = sift.detectAndCompute(template, None)\n",
                "keypoints_images, descriptors_images = [], []\n",
                "for img in images:\n",
                "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
                "    keypoints_images.append(keypoints)\n",
                "    descriptors_images.append(descriptors)\n",
                "\n",
                "# Keypoint matching and homography estimation\n",
                "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
                "homographies = []\n",
                "plt.figure(figsize=(8 * N, 5))\n",
                "plt.suptitle(\"Inlier Keypoint Matches: Template Compared to Different Images with Homography Estimation\")\n",
                "for i, img in enumerate(images):\n",
                "    matches = bf.match(descriptors_template, descriptors_images[i])\n",
                "\n",
                "    # Prepare corresponding point sets for homography estimation\n",
                "    points_template = np.array(\n",
                "        [[keypoints_template[m.queryIdx].pt[0], keypoints_template[m.queryIdx].pt[1], 1] for m in matches]\n",
                "    )\n",
                "    points_image = np.array(\n",
                "        [[keypoints_images[i][m.trainIdx].pt[0], keypoints_images[i][m.trainIdx].pt[1], 1] for m in matches]\n",
                "    )\n",
                "\n",
                "    # Fit homography and find inliers with RANSAC\n",
                "    H, indices_inlier_matches = Ransac_DLT_homography(points_template.T, points_image.T, 3, 1000)\n",
                "    homographies.append(H)\n",
                "\n",
                "    # Show inlier matches\n",
                "    inlier_matches = [matches[idx] for idx in indices_inlier_matches]\n",
                "    img_matched = cv2.drawMatches(\n",
                "        template,\n",
                "        keypoints_template,\n",
                "        img,\n",
                "        keypoints_images[i],\n",
                "        inlier_matches,\n",
                "        None,\n",
                "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
                "    )\n",
                "    plt.subplot(1, N, i + 1)\n",
                "    plt.title(f\"Matching Keypoints of Calibration Template and Image {i + 1}\")\n",
                "    plt.imshow(img_matched)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Compute the Image of the Absolute Conic\n",
                "\n",
                "Given the homographies we are now going to compute the Image of the Absolute conic (IAC).  Read carefully the theory in the provided file 'calibration.pdf'.\n",
                "\n",
                "**Q1.** Complete the code below to compute the IAC from the homographies. Note that $\\Omega$ is `omega_vec` and $\\omega$ is `omega_mat`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute the Image of the Absolute Conic\n",
                "def get_v_ij_T(H, i, j):\n",
                "    \"\"\"\n",
                "    Compute the elements of the v_ij vector for a given homography H and indices i, j.\n",
                "\n",
                "    Parameters:\n",
                "    - H (numpy.ndarray): Homography matrix.\n",
                "    - i (int): Index of the first column of H.\n",
                "    - j (int): Index of the second column of H.\n",
                "\n",
                "    Returns:\n",
                "    - numpy.ndarray: Array containing the elements of the v_ij vector computed from H, i, and j.\n",
                "    \"\"\"\n",
                "    # Initialize array 'a' to store elements of V matrix\n",
                "    a = np.zeros(6)\n",
                "    a[0] = H[0, i] * H[0, j]\n",
                "    a[1] = H[0, i] * H[1, j] + H[1, i] * H[0, j]\n",
                "    a[2] = H[0, i] * H[2, j] + H[2, i] * H[0, j]\n",
                "    a[3] = H[1, i] * H[1, j]\n",
                "    # TODO: Complete\n",
                "    # NOTE: extracted from EQ.12 from calibration.pdf\n",
                "    a[4] = H[1, i] * H[2, j] + H[2, i] * H[1, j]\n",
                "    a[5] = H[2, i] * H[2, j]\n",
                "\n",
                "    return a\n",
                "\n",
                "\n",
                "# Initialize matrix V with zeros\n",
                "V = np.zeros((2 * N, 6))\n",
                "\n",
                "# Loop over each homography and compute corresponding elements of V matrix\n",
                "for h_idx in range(N):\n",
                "    # Select homography matrix corresponding to the current iteration\n",
                "    H = homographies[h_idx]\n",
                "\n",
                "    # Compute elements of V matrix for the current homography and store them in V\n",
                "    V[2 * h_idx, :] = get_v_ij_T(H, i=0, j=1)\n",
                "    V[2 * h_idx + 1, :] = get_v_ij_T(H, i=0, j=0) - get_v_ij_T(H, i=1, j=1)\n",
                "\n",
                "display(V)\n",
                "\n",
                "# TODO: Find omega_vec which has the form [Ï‰_11, Ï‰_12, Ï‰_13, Ï‰_22, Ï‰_23, Ï‰_33]\n",
                "# NOTE: Find solution to V @ omega_vec = 0\n",
                "_, _, Vh = np.linalg.svd(V)\n",
                "omega_vec = Vh[-1, :]\n",
                "\n",
                "# TODO: Find omega_mat, the IAC (Image of the Absolute Conic)\n",
                "# NOTE: fancy numpy way, similar to https://youtu.be/UbmOKqi1mgo?si=KCs1-G-3ziA0MpKU\n",
                "# set upper triangular matrix \n",
                "omega_mat = np.zeros((3, 3))\n",
                "idx, col = np.triu_indices(3, k=0)\n",
                "omega_mat[idx, col] = omega_vec\n",
                "\n",
                "# sum by transpose, removing diagonal\n",
                "omega_mat = omega_mat + np.triu(omega_mat, k=1).T\n",
                "\n",
                "print(\"omega_vec:\", omega_vec)\n",
                "print(\"\\nomega_mat:\", omega_mat)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
                "    <strong>ðŸŽ¥ Video Question 2:</strong><br>\n",
                "    <li>How many images are required to compute the IAC? Why?<br>\n",
                "    <li>What would you do if you had fewer images?\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Compute the camera calibration matrix from the IAC\n",
                "\n",
                "The IAC relates to the camera calibration matrix, $K$ as $\\omega = K^{-T} K^{-1}$. Knowing $\\omega$ (`omega_mat`) we can get $K$ using the Cholesky factorization.\n",
                "\n",
                "**Q3.** Write the code to compute the camera calibration matrix from the IAC $\\omega$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find K with Cholesky\n",
                "# NOTE: from theory class, omega_mat = inv(KK') --> KK' = inv(omega_mat)\n",
                "# NOTE: WHY DOES THIS NOT WORK !!!\n",
                "# K = cholesky(np.linalg.inv(omega_mat), lower=True).T\n",
                "# K = cholesky(np.linalg.inv(omega_mat))\n",
                "\n",
                "K = np.linalg.inv(cholesky(omega_mat))\n",
                "K /= K[-1, -1]\n",
                "\n",
                "print(\"Estimated camera calibration matrix K:\", K, sep=\"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>ðŸŽ¥ Video Question (Q4):</strong>\n",
                "  <ul>\n",
                "    <li>What result do you get?</li>\n",
                "    <li>Can you interpret the values of <span style=\"font-family: 'Times New Roman', serif;\">K</span> in geometric terms?</li>\n",
                "    <li>Which ones are related to the image size? How are they related? Do they make sense in your results?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q5.** Compute the field of view angle of the camera."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "'''\n",
                "K = [[a_x   s   p_X],\n",
                "     [ 0   a_Y  p_Y],\n",
                "     [ 0    0    1 ]]\n",
                "\n",
                "cos(fov_X / 2) = p_X / a_X --> arccos(p_X / a_X) = fov_X / 2 --> fov_X = arccos(p_X / a_X) * 2\n",
                "wrong it should be tan\n",
                "'''\n",
                "# NOTE: NOT cosine --> tangent\n",
                "# NOTE: get width and height from the image itself, not from K\n",
                "\n",
                "fov_x_deg = np.rad2deg(np.arctan(template.shape[0] / K[0, 2]) * 2)\n",
                "fov_y_deg = np.rad2deg(np.arctan(template.shape[1] / K[1, 2]) * 2)\n",
                "print(f\"Estimated horizontal FOV (x): {fov_x_deg:.2f} degrees\")\n",
                "print(f\"Estimated vertical FOV (y): {fov_y_deg:.2f} degrees\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 Compute camera position and orientation\n",
                "\n",
                "Given the computed calibration $K$ and the homografies $H$, we can compute the 3D position and orientation of the camera with respect to the planar pattern.\n",
                "\n",
                "**Q6.** Complete the code to compute the rotation and translation of each camera.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute camera position and orientation\n",
                "rotations = []\n",
                "translations = []\n",
                "projection_matrices = []\n",
                "\n",
                "for img_idx in range(N):\n",
                "    H = homographies[img_idx]\n",
                "\n",
                "    # TODO: Compute r1, r2, t\n",
                "    r1 = ...\n",
                "    r2 = ...\n",
                "    t = ...\n",
                "\n",
                "    # Solve scale ambiguity\n",
                "    s = math.sqrt(np.linalg.norm(r1) * np.linalg.norm(r2)) * np.sign(t[2])\n",
                "    r1 = r1 / s\n",
                "    r2 = r2 / s\n",
                "    t = t / s\n",
                "    translations.append(t)\n",
                "\n",
                "    # TODO: Compute r3\n",
                "    r3 = ...\n",
                "\n",
                "    # Assemble rotation matrix\n",
                "    R = np.column_stack((r1, r2, r3)) \n",
                "\n",
                "    # Orthonormalize R with SVD to make it a valid rotation matrix\n",
                "    # i.e., approximate R to a rotation matrix with unitary singular values\n",
                "    U, _, Vt = np.linalg.svd(R)\n",
                "    R = U @ np.identity(3) @ Vt\n",
                "    rotations.append(R)\n",
                "\n",
                "    # TODO: Compute projection matrix P = K [R | t]\n",
                "    extrinsics = np.column_stack((???, ???.reshape(-1, 1)))\n",
                "    P = ...\n",
                "    projection_matrices.append(P)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>ðŸŽ¥ Video Question (Q7):</strong>\n",
                "  <ul>\n",
                "    <li>In which units is the translation vector <b>t</b> expressed?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following code visualizes, in 3D, the different cameras corresponding to each image, along with the rectangular frame of the planar pattern. Each camera is represented as a pyramid, where the apex indicates the estimated camera position, and the base reflects its orientation with respect to the planar scene."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_h, img_w = images[0].shape\n",
                "\n",
                "fig = go.Figure()\n",
                "for img_idx in range(N):\n",
                "    plot_camera(projection_matrices[img_idx], img_w, img_h, fig, f\"Camera {img_idx+1}\")\n",
                "\n",
                "plot_image_origin(img_w/2, img_h/2, fig, \"Image\")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q8.** Complete the code below to plot a moving planar target in front of a static camera.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_image_Rt(R, t, w, h, fig, legend):\n",
                "    \"\"\"\n",
                "    Plot the camera pose and image plane in 3D space.\n",
                "\n",
                "    Parameters:\n",
                "        R (numpy.ndarray): Rotation matrix.\n",
                "        t (numpy.ndarray): Translation vector.\n",
                "        w (float): Width of the image plane.\n",
                "        h (float): Height of the image plane.\n",
                "        fig: Plotly figure object.\n",
                "        legend (str): Legend label for the plot.\n",
                "\n",
                "    Returns:\n",
                "        None\n",
                "    \"\"\"\n",
                "\n",
                "    # TODO: Define the vertices of the image plane\n",
                "    p1 = ...\n",
                "    p2 = ...\n",
                "    p3 = ...\n",
                "    p4 = ...\n",
                "\n",
                "    \n",
                "    # TODO: Project the vertices to the camera coordinates\n",
                "\n",
                "    # Extract coordinates for plotting\n",
                "    x = np.array([p1[0], p2[0], p3[0], p4[0], p1[0]])\n",
                "    y = np.array([p1[1], p2[1], p3[1], p4[1], p1[1]])\n",
                "    z = np.array([p1[2], p2[2], p3[2], p4[2], p1[2]])\n",
                "\n",
                "    fig.add_trace(go.Scatter3d(x=x, y=z, z=-y, mode=\"lines\", name=legend))\n",
                "\n",
                "    return\n",
                "\n",
                "\n",
                "fig = go.Figure()\n",
                "A = np.column_stack((np.identity(3), np.zeros(3)))\n",
                "plot_camera(K @ A, img_w, img_h, fig, \"Camera\")\n",
                "for i in range(N):\n",
                "    plot_image_Rt(rotations[i], translations[i], img_w / 2, img_h / 2, fig, f\"Image {i+1}\")\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>ðŸŽ¥ Video Question (Q9):</strong>\n",
                "  <ul>\n",
                "    <li>When taking the images, we moved the camera and the planar pattern was static. The previous 3D plot shows a moving pattern in front of a static camera.  What is the relation between the two situations?</li>\n",
                "    <li> Do the results make sense to you?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.5 Augmented reality\n",
                "\n",
                "We now have the projection matrices, $P$, of each image with respect to the planar target's reference frame. We can use these projection matrices for projecting 3D points onto the images. Given a 3D point, we can project it using the projection matrix and plot the projection over the image.\n",
                "\n",
                "**Q10.** Complete the code below to plot a cube in front of the calibration pattern on each image. The result should look as if a cube was attached to the pattern as the pattern moves from image to image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the vertices of the cube\n",
                "cube_vertices = np.array(\n",
                "    [\n",
                "        [0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0],  # Front vertices\n",
                "        [0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1],  # Back vertices\n",
                "    ],\n",
                "    dtype=np.float64,\n",
                ").T\n",
                "\n",
                "# Translate the vertices to the center of the back face\n",
                "cube_backface_centre = np.array([[0.5, 0.5, 1]]).T\n",
                "cube_vertices -= cube_backface_centre\n",
                "\n",
                "# Scale the vertices based on the width of the template image\n",
                "template_height, template_width = template.shape\n",
                "scaling_factor = template_width / 4\n",
                "cube_vertices *= scaling_factor\n",
                "\n",
                "# Translate the vertices to the center of the template image\n",
                "image_centre = np.array([[template_width / 2, template_height / 2, 0]]).T\n",
                "cube_vertices += image_centre\n",
                "\n",
                "# Convert vertices to homogeneous coordinates\n",
                "cube_vertices = np.vstack((cube_vertices, np.ones((1, cube_vertices.shape[1]))))\n",
                "\n",
                "# Define edges of the cube\n",
                "cube_edges_idxs = [\n",
                "    [0, 1], [1, 2], [2, 3], [3, 0],  # Front face edges\n",
                "    [4, 5], [5, 6], [6, 7], [7, 4],  # Back face edges\n",
                "    [0, 4], [1, 5], [2, 6], [3, 7],  # Connecting edges between Front and Back faces\n",
                "]\n",
                "\n",
                "# Define line color for drawing edges\n",
                "LINE_COLOR = (0, 255, 0)\n",
                "\n",
                "# Create a plot for each image\n",
                "plt.figure(figsize=(10 * N, 8))\n",
                "for i in range(N):\n",
                "    # TODO: Project vertices onto the image plane using the projection matrix\n",
                "    # Remember to normalize to homogeneous coordinates and convert it to pixels (integers, not floats)\n",
                "    xp = ...\n",
                "\n",
                "    # Read the original image\n",
                "    original_image = cv2.imread(\"Data/image{0}.jpg\".format(i + 1), cv2.IMREAD_COLOR)\n",
                "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "    # Fill the back face of the cube with 30% transparency\n",
                "    image = original_image.copy()\n",
                "    xp_backface = xp[:2, -4:].T\n",
                "    image = cv2.fillPoly(image, pts=[xp_backface], color=LINE_COLOR)\n",
                "    alpha = 0.3\n",
                "    image = cv2.addWeighted(src1=image, alpha=alpha, src2=original_image, beta=1 - alpha, gamma=0)\n",
                "\n",
                "    # Draw cube edges\n",
                "    for vertex_a_idx, vertex_b_idx in cube_edges_idxs:\n",
                "        image = cv2.line(image, (xp[:2, vertex_a_idx]), (xp[:2, vertex_b_idx]), LINE_COLOR, 8)\n",
                "\n",
                "    # Draw a circle at the center of the back face\n",
                "    backface_centre = np.mean(xp[:, (4, 6)], axis=1, dtype=np.int32)\n",
                "    image = cv2.circle(image, backface_centre[:2], radius=5, color=LINE_COLOR, thickness=20)\n",
                "\n",
                "    # Show the image with the projected cube\n",
                "    plt.subplot(1, N, i + 1)\n",
                "    plt.imshow(image)\n",
                "    plt.title(f\"Projected Cube in front of Image {i+1}\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q11.** Repeat all the process using your own images (you may use from 3 to 5 different views of a planar pattern). Comment the results and try to find an explanation in case it doesn't work.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Optional answer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### 2.6 Reducing the number of images\n",
                "\n",
                "In case we have some knowledge about the camera parameters like the principal point, the aspect ratio or zero skew factor the number of images needed for calibration can be reduced.\n",
                "\n",
                "**Q12. (Optional)** Add a linear constraint to enforce skew 0.  See Zhang's paper (zhang98.pdf) [1]. What is the minimum number of views you need to calibrate the camera in this case?\n",
                "\n",
                "[1] Zhengyou Zhang. A flexible new technique for camera calibration, IEEE Transactions on pattern analysis and machine intelligence, 22(11), 1330-1334, 2000.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Optional answer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q13. (Optional)** Can you calibrate from a single image? How? Adapt your code to be able to calibrate from just a single image. Compare the results with the ones obtained when using more images. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Optional answer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q14. Optional:** Using the last result you can create a video where the projected virtual cube automatically adapts to the changing point of view when you move the camera.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Optional answer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. References"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Add here the material you used to complete this Lab. Cite and describe the usage of AI tools if any was used according to the Guidelines for AI tools."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TODO: Complete"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>ðŸŽ¥ Video Questions</strong>: Briefly mention the references.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>ðŸŽ¥ Self-Assessment and Conclusions</strong>:\n",
                "  <ul>\n",
                "  <li><b>Which parts of the notebook did you succeed in? </b><br>\n",
                "  <em>Describe the sections where you felt confident, and explain why you think they were successful.</em></li>\n",
                "  <li><b>Which parts of the notebook did you fail to solve? </b><br>\n",
                "  <em>Be honest about the areas where you faced difficulties. What challenges or issues did you encounter that you couldnâ€™t resolve? How would you approach these issues in the future?</em></li>\n",
                "  </ul>\n",
                "  Is there anything else that you would like to comment?\n",
                "</div>\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
