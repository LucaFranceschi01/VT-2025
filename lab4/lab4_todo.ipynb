{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Students\n",
                "- Student 1: <span style=\"color:green\">253885 - Luca Franceschi</span>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 4: Stereo Matching and Shape from Silhouette\n",
                "\n",
                "This lab session is divided in two parts. In the first one, you will compute a disparity map from a pair of images.  A disparity map is an image that stores the inverse of the distance to the camera center of the points\n",
                "seen at every pixel, i.e. the depth.  This will enable us, for example in case our cameras are calibrated, to display a dense cloud of points representing the scene.\n",
                "\n",
                "To compute the disparity map with a local method, we will have to compute correspondences for every pixel in the reference image.  These are many more correspondences than the ones given by SIFT/SURF/ORB.  For every pixel in the first image, we will travel its corresponding epipolar line in the second image looking for its correspondence. Since the pair of images are stereo-rectified (parallel views) the epipolar lines are horizontal and the coordinates of the corresponding points differ just by a horizontal displacement, the disparity.  By comparing the information around a neighborhood of the pixels on both images we will decide whether the pixels do correspond to each other.\n",
                "\n",
                "In the second part, you will compute a 3D reconstruction of an object given some binary images corresponding to different points of view. These binary images, called the silhouettes, contain a segmentation of the object of interest that we want to reconstruct.\n",
                "\n",
                "In particular, the goal of this lab session is to learn the following concepts:\n",
                "\n",
                "- How to compute disparity/depth maps from pairs of images.\n",
                "- Compare different cost functions and window sizes.\n",
                "- How to aggregate costs through bilateral weights.\n",
                "- How to extract a Visual Hull from a set of different views.\n",
                "\n",
                "You will have to answer the questions and complete the provided code when necessary as required. **You must deliver the completed (and executed) ipynb file, including the answers to the questions (please make clear visually what it is answer, either preceding it by ANSWER and/or changing its color).**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "import cv2\n",
                "import ipyvolume as ipv\n",
                "import matplotlib\n",
                "import numpy as np\n",
                "import seaborn_image as isns\n",
                "from ipywidgets import IntSlider, interact\n",
                "from matplotlib import pyplot as plt\n",
                "from PIL import Image, ImageDraw\n",
                "from tqdm.notebook import tnrange, tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Stereo matching\n",
                "\n",
                "First we will work with a pair of stereo-recified images from the Middlebury stereo dataset: http://vision.middlebury.edu/stereo/. The ground truth disparity is available and it will be useful to validate the code to be completed and it will allow us to measure errors as well.\n",
                "\n",
                "The incomplete function `stereo_computation` is provided; you have to complete it with the SSD cost, the NCC similarity measure and the bilateral weights.\n",
                "\n",
                "**Q1.** Complete the `stereo_computation` function with the computation of the SSD cost.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def stereo_computation(\n",
                "    color_left_img, color_right_img, min_disparity, max_disparity, win_size, bilateral_weights=False, method=\"ssd\"\n",
                "):\n",
                "    \"\"\"\n",
                "    -left image\n",
                "    -right image\n",
                "    -minimum disparity\n",
                "    -maximum disparity\n",
                "    -window size\n",
                "    -bilateral_weights\n",
                "    -matching cost\n",
                "    \"\"\"\n",
                "\n",
                "    # conversion to float images to make calculations\n",
                "    color_left_img = color_left_img.astype(np.float32) # NOTE: rename to reference / target, this one is reference\n",
                "    color_right_img = color_right_img.astype(np.float32)\n",
                "\n",
                "    # complete ...\n",
                "    height, width, _ = color_left_img.shape\n",
                "    estimated_disparity_map = np.zeros(shape=(height, width))\n",
                "    matching_cost = np.zeros((max_disparity - min_disparity))\n",
                "    half_win_size = win_size // 2\n",
                "    \n",
                "    # NOTE: initialize here, always three channels even though not really used in ssd\n",
                "    weights = np.zeros((win_size, win_size, 3)) \n",
                "        \n",
                "    # iterate over the image\n",
                "    description = f\"Stereo computation: {method.upper()} \\\n",
                "with {win_size}x{win_size} window \\\n",
                "and {'bilateral' if bilateral_weights else 'uniform'} weights\"\n",
                "    \n",
                "    for i in tnrange(half_win_size, height - half_win_size, desc=description):\n",
                "        for j in range(half_win_size, width - half_win_size):\n",
                "            \n",
                "            # TODO: Define the window for the reference image\n",
                "            # NOTE: IT IS (HEIGHT, WIDTH) !!! too used to (width, height)\n",
                "            # NOTE: window centered at position (i, j) and size (height, width)\n",
                "            window_img_1 = color_left_img[i - half_win_size : i + half_win_size + 1,\n",
                "                                           j - half_win_size : j + half_win_size + 1]\n",
                "\n",
                "            # TODO: Define the disparity range\n",
                "            disparity_range = range(min_disparity, max_disparity)\n",
                "\n",
                "            if bilateral_weights:\n",
                "                # TODO: ANSWER Q7 - Code for bilateral weights.\n",
                "                # for p in range(width):\n",
                "                #     for q in range(height):\n",
                "                raise NotImplementedError(\"bilateral weights not implemented\")\n",
                "\n",
                "            else:\n",
                "                # TODO: Code for uniform weights.\n",
                "                weights[:, :, :] = 1 / (win_size * win_size * 3)\n",
                "\n",
                "            for d in disparity_range:\n",
                "                # TODO: Ensure that the disparity does not go out of bounds\n",
                "                new_center = j + d\n",
                "                if (new_center + half_win_size > width - 1):\n",
                "                    continue\n",
                "                # left boundary\n",
                "\n",
                "                # TODO: Define the window for the second image\n",
                "                window_img_2 = color_right_img[i - half_win_size : i + half_win_size + 1,\n",
                "                            new_center - half_win_size : new_center + half_win_size + 1]\n",
                "\n",
                "                if method == \"ssd\":\n",
                "                    # TODO: ANSWER Q1\n",
                "                    matching_cost[d] = np.sum((window_img_1 - window_img_2)**2)\n",
                "\n",
                "                elif method == \"ncc\":\n",
                "                    # TODO: ANSWER Q5\n",
                "                    raise NotImplementedError(\"ncc not implemented\")\n",
                "\n",
                "                else:\n",
                "                    raise (NameError)\n",
                "            \n",
                "            # TODO: Update disparity map\n",
                "            estimated_disparity_map[i, j] = np.argmin(matching_cost)\n",
                "\n",
                "    return estimated_disparity_map"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q2.** Execute cell code given after the `stereo_computation` function. This code estimates the disparity between a pair of stereo rectified images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data images (rectified images)\n",
                "img1 = cv2.imread(\"./data1/scene1.row3.col3.ppm\")\n",
                "img2 = cv2.imread(\"./data1/scene1.row3.col2.ppm\")\n",
                "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
                "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.suptitle(\"Stereo rectified images\")\n",
                "for i, (image, title) in enumerate(zip([img2, img1], [\"Left image\", \"Right image (reference)\"]), start=1):\n",
                "    plt.subplot(1, 2, i)\n",
                "    plt.imshow(image)\n",
                "    plt.title(title)\n",
                "    plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set minimum and maximum disparity\n",
                "min_disparity = 0\n",
                "max_disparity = 16\n",
                "\n",
                "# Set window (patch) size\n",
                "w_size = 9\n",
                "ssd_disp = np.zeros(shape=(img1.shape[0], img1.shape[1]))\n",
                "ssd_disp = stereo_computation(img1, img2, min_disparity, max_disparity, w_size, bilateral_weights=False, method=\"ssd\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Disparity ground truth image\n",
                "img_gt = cv2.imread(\"./data1/truedisp.row3.col3.pgm\", cv2.IMREAD_GRAYSCALE)\n",
                "img_gt = img_gt / 16.0\n",
                "\n",
                "# Display the estimated image with respect to the ground truth\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.suptitle(\"Disparity maps\")\n",
                "for i, (image, title) in enumerate(zip([img_gt, ssd_disp], [\"Ground Truth\", \"SSD estimation\"]), start=1):\n",
                "    ax = plt.subplot(1, 2, i)\n",
                "    plt.title(title)\n",
                "    isns.imshow(image, ax=ax, gray=True, vmax=max(np.max(img_gt), max_disparity))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q2):</strong>\n",
                "  <ul>\n",
                "    <li>What is the relationship between estimation errors and occluded regions? Why?</li>\n",
                "    <li>Show an example in the plot above.</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q3.** Compute the global error between the ground truth and the estimated disparity map. You should apply the MSE.\n",
                "\n",
                "*Note that we want to compare only the disparity where the ground truth has values different from zero. I.e. we don't want to compare the borders of the disparity maps, where the ground truth has zero values (in black above). Thus, you should apply some kind of masking.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_mse(ground_truth_disp_map, estimated_disp_map):\n",
                "    # TODO: Compute the mean squared error (MSE) between the estimated disparity map and the ground truth disparity map\n",
                "\n",
                "print(f\"MSE: {compute_mse(img_gt, ssd_disp):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q4.** Evaluate the results changing the window size (e.g. $5 \\times 5$, $9 \\times 9$, $21 \\times 21$). Show the results in 3 plots side by side.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CODE ANSWER Q4\n",
                "ssd_disp_per_window = {}\n",
                "\n",
                "# TODO: Compute the disparity map for different window sizes\n",
                "for w_size in [...]:\n",
                "    ssd_disp_per_window[f\"{w_size}x{w_size}\"] = stereo_computation(...)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the estimated image with respect to the ground truth\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.suptitle(\"Disparity maps with SSD and uniform weights\")\n",
                "for i, (image, title) in enumerate(\n",
                "    zip(\n",
                "        [img_gt, *ssd_disp_per_window.values()],\n",
                "        [\"Ground Truth\", *[f\"Estimation with {k} window\" for k in ssd_disp_per_window.keys()]],\n",
                "    ),\n",
                "    start=1,\n",
                "):\n",
                "    ax = plt.subplot(2, 2, i)\n",
                "    plt.title(title)\n",
                "    isns.imshow(image, ax=ax, gray=True, vmax=max(np.max(img_gt), max_disparity))\n",
                "    \n",
                "    # TODO: Compute the mse\n",
                "\n",
                "    plt.legend([f\"MSE = {mse:.2f}\"], loc=\"upper center\", bbox_to_anchor=(-21, 1))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q4):</strong>\n",
                "  <ul>\n",
                "    <li>What are the key differences between the disparity maps, and how do they relate to window size?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q5.** Complete the `stereo_computation` function with the computation of the NCC cost (In the first code cell).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q6.** Evaluate the results changing the window size (e.g. $5 \\times 5$, $9 \\times 9$, $21 \\times 21$). Show the results in 3 plots side by side.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CODE ANSWER Q6\n",
                "ncc_disp_per_window = {}\n",
                "\n",
                "# TODO: Compute the disparity map for different window sizes\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12, 8))\n",
                "plt.suptitle(\"Disparity maps with NCC and uniform weights\")\n",
                "for i, (image, title) in enumerate(\n",
                "    zip(\n",
                "        [img_gt, *ncc_disp_per_window.values()],\n",
                "        [\"Ground Truth\", *[f\"Estimation with {k} window\" for k in ncc_disp_per_window.keys()]],\n",
                "    ),\n",
                "    start=1,\n",
                "):\n",
                "    ax = plt.subplot(2, 2, i)\n",
                "    plt.title(title)\n",
                "    isns.imshow(image, ax=ax, gray=True, vmax=max(np.max(img_gt), max_disparity))\n",
                "    mse = compute_mse(img_gt, image)\n",
                "    plt.legend([f\"MSE = {mse:.2f}\"], loc=\"upper center\", bbox_to_anchor=(-21, 1))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q6):</strong>\n",
                "  <ul>\n",
                "    <li>Again, what are the key differences between the disparity maps, and how do they relate to window size?</li>\n",
                "    <li>Is there any difference compared to the SSD results?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q7.** Complete the `stereo_computation` function from the first code cell with the bilateral weights. Suggested parameters: $\\gamma_{col}=12$ and $\\gamma_{pos} = win\\_size // 2$.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q7):</strong>\n",
                "  <ul>\n",
                "    <li>Briefly explain how bilateral weights work.</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q8.** Evaluate bilateral weights changing the window size (e.g. $5 \\times 5$, $9 \\times 9$, $21 \\times 21$) and compare to the previous case that uses uniform weights (SDD cost)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CODE ANSWER Q8\n",
                "ssd_bilat_disp_per_window = {}\n",
                "\n",
                "# TODO: Compute the disparity map for different window sizes\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the estimated image with respect to the ground truth\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.suptitle(\"Disparity maps with SSD and bilateral windows\")\n",
                "for i, (image, title) in enumerate(zip([img_gt, *ssd_bilat_disp_per_window.values()], [\"Ground Truth\", *[f\"Estimation with {k} window\" for k in ssd_disp_per_window.keys()]]), start=1):\n",
                "    ax = plt.subplot(2, 2, i)\n",
                "    plt.title(title)\n",
                "    isns.imshow(image, ax=ax, gray=True, vmax=max(np.max(img_gt), max_disparity))\n",
                "    \n",
                "    # TODO: Compute the mse\n",
                "\n",
                "    plt.legend([f\"MSE = {mse:.2f}\"], loc='upper center', bbox_to_anchor=(-21,1))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q6):</strong>\n",
                "  <ul>\n",
                "    <li>Comment the results. Compare them with SSD with uniform weights.</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Once the code is completed and you finish the experiments with the Middlebury image, the next step is to apply the code to our lab 4 images. These images are not rectified, we have rectified them by applying a proper homography to each one of them. The stereo-rectified images are the ones provided in this lab. The lab images are big. To speed up the computations, you can scale-down the images by a factor of 0.25. Once everything is working properly, you can increase the scale in order to get higher resolution results.\n",
                "\n",
                "**Q9.** Set appropriate values for the disparity limits."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data images (rectified images)\n",
                "img1 = cv2.imread(\"./data1/rectif0.png\")\n",
                "img2 = cv2.imread(\"./data1/rectif1.png\")\n",
                "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
                "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "# Showing images\n",
                "plt.figure(figsize=(15, 5))\n",
                "plt.suptitle(\"Rectified images from Lab 3\")\n",
                "for i, (image, title) in enumerate(zip([img1, img2], [\"Left image\", \"Right image\"]), start=1):\n",
                "    ax = plt.subplot(1, 2, i)\n",
                "    plt.title(title)\n",
                "    isns.imshow(image, ax=ax)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RESIZE FOR A LIGHTER COMPUTATION\n",
                "# Calculate the new dimensions\n",
                "scaling_factor = 0.25\n",
                "new_width = int(img1.shape[1] * scaling_factor)\n",
                "new_height = int(img1.shape[0] * scaling_factor)\n",
                "new_dim = (new_width, new_height)\n",
                "\n",
                "# Resize the images\n",
                "resized_img1 = cv2.resize(img1, new_dim, interpolation=cv2.INTER_AREA)\n",
                "resized_img2 = cv2.resize(img2, new_dim, interpolation=cv2.INTER_AREA)\n",
                "\n",
                "# TODO: ANSWER Q9 resized\n",
                "min_disparity = ...\n",
                "max_disparity = ...\n",
                "\n",
                "ssd_lab3_resized = {}\n",
                "\n",
                "# TODO: Compute the disparity map for different window sizes\n",
                "for w_size in [???, ???, ???]:\n",
                "    ssd_lab3_resized[f\"{w_size}x{w_size}\"] = stereo_computation(???)\n",
                "\n",
                "# Display the estimated disparity maps\n",
                "plt.figure(figsize=(16, 5))\n",
                "plt.suptitle(\"Disparity maps with SSD and bilateral windows\")\n",
                "for i, (window, image) in enumerate(ssd_lab3_resized.items(), start=1):\n",
                "    ax = plt.subplot(1, 3, i)\n",
                "    plt.title(f\"Estimation with {window} window\")\n",
                "    isns.imshow(image, ax=ax, gray=True, vmax=max_disparity)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ANSWER Q9 with original size\n",
                "# Hint: It will take time to run the code with original size, so pick one window size that you think that will work based on the resized images.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q10):</strong>\n",
                "  <ul>\n",
                "    <li>Comment the result obtained qualitatively.</li>\n",
                "    <li>How did you change the disparity limits when using original image sizes?</li>\n",
                "    <li>Is the disparity well estimated all around the image?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Shape from silhouette: visual hull method\n",
                "\n",
                "The Visual Hull is a 3D reconstruction technique that allows to recover a 3D shape given its silhouettes in different views. A silhouette is a mask that provides a segmentation of the foreground object. The Visual Hull may be obtained as the intersection of the visual cones of the different points of view, which are generated by backprojection of the silhouttes using camera parameters. The advantage of this method is that it is very simple, it is non-iterative and can be easily parallelized. Its main disadvantage is the inability to obtain the exact shape of the object (e.g. cavities in the object cannot be recovered) but it is widely used with multi-view stereo algorithms, either as a first step or in conjunction with the multi-view reconstruction providing additional constraints.\n",
                "In practice, the visual cones are not estimated, instead, a voxel grid is defined and all the voxels of the grid are projected onto each image ir order to check if the voxel projects inside the object silhouette or not.\n",
                "\n",
                "In order to visualize the 3D volume we will use the Python library IPyvolume: https://ipyvolume.readthedocs.io/en/latest/ \n",
                "\n",
                "You will have to install the library, more information here: https://ipyvolume.readthedocs.io/en/latest/install.html\n",
                "\n",
                "In our case it worked with the following commands:\n",
                "\n",
                "- `pip install ipyvolume`\n",
                " \n",
                "- `jupyter nbextension enable --py --sys-prefix ipyvolume` \n",
                " \n",
                "- `jupyter nbextension enable --py --sys-prefix widgetsnbextension`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Execute the code in the cell below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Shape from silhouette - Visual Hull algorithm\n",
                "\n",
                "# Load images and projection matrices\n",
                "numCameras = 18\n",
                "original_images = [cv2.imread(\"data2/david_{:02d}.jpg\".format(i), cv2.IMREAD_GRAYSCALE) for i in range(numCameras)]\n",
                "P_matrices = [np.loadtxt(\"data2/david_{:02d}.pa\".format(i), delimiter=\" \") for i in range(numCameras)]\n",
                "\n",
                "# Compute silhouettes\n",
                "silhouetteThreshold = 100\n",
                "silhouettes = [cv2.threshold(im, silhouetteThreshold, 255, cv2.THRESH_BINARY)[1] for im in original_images]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q11):</strong>\n",
                "  <ul>\n",
                "    <li>How are the silhouettes extracted in the provided code?</li>\n",
                "    <li>Would it work in a general case? Why?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define function to display images and silhouettes\n",
                "def display_image_with_silhouette(num_image):\n",
                "    i = num_image\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
                "\n",
                "    # Display original image\n",
                "    axes[0].imshow(original_images[i], cmap=\"gray\")\n",
                "    axes[0].set_title(f\"Image {i:02d}\")\n",
                "    axes[0].axis(\"off\")\n",
                "\n",
                "    # Display silhouette\n",
                "    axes[1].imshow(silhouettes[i], cmap=\"gray\")\n",
                "    axes[1].set_title(f\"Silhouette {i:02d}\")\n",
                "    axes[1].axis(\"off\")\n",
                "\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "# Create slider\n",
                "slider = IntSlider(min=0, max=numCameras - 1, step=1, value=0)\n",
                "\n",
                "# Display interactive slider and images\n",
                "display(interact(display_image_with_silhouette, num_image=slider))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Execute the following cell code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bounding box of the volume in 3D space.\n",
                "# This bounding box represents the region where the visual hull reconstruction will take place.\n",
                "bbox = np.array([[0.2, -0.3, -1.8], [2.2, 1.3, 2.6]])  # [minX minY minZ; maxX maxY maxZ]\n",
                "\n",
                "# Volume Resolution\n",
                "# These variables determine the size of the voxel grid used for the visual hull reconstruction. Larger values result in higher resolution but require more computational resources.\n",
                "volumeX = 40  # 64 # start with a small volume and once code works properly you can increase its size\n",
                "volumeY = 40  # 64\n",
                "volumeZ = 80  # 128\n",
                "\n",
                "# Transformation Matrix (T)\n",
                "# T is a transformation matrix that maps coordinates from the bounding box space to the image space.\n",
                "T = np.identity(4)\n",
                "T[:3, 3] = bbox[0, :]\n",
                "T = T @ np.diag(\n",
                "    [(bbox[1, 0] - bbox[0, 0]) / volumeX, (bbox[1, 1] - bbox[0, 1]) / volumeY, (bbox[1, 2] - bbox[0, 2]) / volumeZ, 1]\n",
                ")\n",
                "F = np.array(\n",
                "    [[1, 0, 0, 0], [0, 0, 1, 0], [0, -1, 0, 0], [0, 0, 0, 1]]\n",
                ")  # flip y and z axes for better display in isosurface\n",
                "T = F @ T\n",
                "\n",
                "# Corner Points\n",
                "# corners represent the eight corner points of the voxel grid in the voxel grid space.\n",
                "corners = np.array(\n",
                "    [\n",
                "        [0, 0, 0, 1],\n",
                "        [0, 0, volumeZ, 1],\n",
                "        [0, volumeY, 0, 1],\n",
                "        [0, volumeY, volumeZ, 1],\n",
                "        [volumeX, 0, 0, 1],\n",
                "        [volumeX, 0, volumeZ, 1],\n",
                "        [volumeX, volumeY, 0, 1],\n",
                "        [volumeX, volumeY, volumeZ, 1],\n",
                "    ]\n",
                ")\n",
                "corners = corners.T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define function to display images and silhouettes\n",
                "def display_image_with_silhouette(num_image):\n",
                "    i = num_image\n",
                "    pcorners = P_matrices[i] @ T @ corners\n",
                "    pcorners = pcorners[:2] / pcorners[2:]  # From P^2 to R^2\n",
                "    isns.imshow(original_images[i], cbar=False, gray=True)\n",
                "    plt.scatter(x=pcorners[0, :], y=pcorners[1, :], c=\"yellow\", s=10)\n",
                "    plt.title(f\"image {i:02d}\")\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "# Create slider\n",
                "slider = IntSlider(min=0, max=numCameras - 1, step=1, value=0)\n",
                "\n",
                "# Display interactive slider and images\n",
                "display(interact(display_image_with_silhouette, num_image=slider))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q12):</strong>\n",
                "  <ul>\n",
                "    <li>What is the projection of volume corners useful for?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "The following cell contains the visual hull algorithm with a missing part that needs to be completed.\n",
                "\n",
                "**Q13.** Complete the missing part of the code and execute the final cell to visualize the 3D reconstruction. For every camera, you will have to project every voxel into the image plane and check if it is projected within the white silhouette. If it lies within the silhouette increment the counter for that voxel `volume[x, y, z]` by 1. `volume[x, y, z]` will count the number of views where the voxel will be projected into the silhouette."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visual Hull computation\n",
                "\n",
                "# Initialize the volume grid with zeros\n",
                "volume = np.zeros((volumeX, volumeY, volumeZ))\n",
                "\n",
                "# Define the coordinates of the center of each voxel\n",
                "x = np.arange(0.5, volumeX, 1)\n",
                "y = np.arange(0.5, volumeY, 1)\n",
                "z = np.arange(0.5, volumeZ, 1)\n",
                "voxel3Dx, voxel3Dy, voxel3Dz = np.meshgrid(x, y, z)\n",
                "\n",
                "# Get the dimensions of the silhouette images\n",
                "ny, nx = silhouettes[0].shape\n",
                "\n",
                "# Loop through each camera\n",
                "for n in tnrange(numCameras, desc=\"Visual Hull Algorithm\"):\n",
                "    # Iterate over each voxel in the volume\n",
                "    for x in range(volumeX):\n",
                "        for y in range(volumeY):\n",
                "            for z in range(volumeZ):\n",
                "                # Get the 3D coordinates of the voxel\n",
                "                voxel = np.array([voxel3Dx[x, y, z], voxel3Dy[x, y, z], voxel3Dz[x, y, z], 1])\n",
                "                # Transform the voxel coordinates from volume space to world space\n",
                "                world_coords = T @ voxel.T\n",
                "\n",
                "                # ANSWER Q13\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To render the volume, we will show its surface. The surface is defined by the voxels that lie within the silhouette for all cameras except one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "volumeThreshold = numCameras - 1\n",
                "ipv.figure()\n",
                "ipv.plot_isosurface(volume, level=volumeThreshold, color=\"gray\", extent=[[0.2, 2.2], [-0.3, 1.3], [-1.8, 2.7]])\n",
                "ipv.squarelim()\n",
                "ipv.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Question (Q14):</strong>\n",
                "  <ul>\n",
                "    <li>Explain how the visual hull algorithm works</li>\n",
                "    <li>Check the bottom of this sculpture, is it void inside? Why?</li>\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "-------------------"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. References"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Add here the material you used to complete this Lab. Cite and describe the usage of AI tools if any was used according to the Guidelines for AI tools."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TODO: Complete"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Video Questions</strong>: Briefly mention the references.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: rgba(255, 255, 255, 0);\">\n",
                "  <strong>🎥 Self-Assessment and Conclusions</strong>:\n",
                "  <ul>\n",
                "  <li><b>Which parts of the notebook did you succeed in? </b><br>\n",
                "  <em>Describe the sections where you felt confident, and explain why you think they were successful.</em></li>\n",
                "  <li><b>Which parts of the notebook did you fail to solve? </b><br>\n",
                "  <em>Be honest about the areas where you faced difficulties. What challenges or issues did you encounter that you couldn’t resolve? How would you approach these issues in the future?</em></li>\n",
                "  </ul>\n",
                "  Is there anything else that you would like to comment?\n",
                "</div>\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
