{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 1: Planar Transformations, Homography Estimation & Image Mosaics\n",
                "\n",
                "Welcome to your first hands-on lab! In this exercise, we'll dive into the fascinating world of image transformations and learn how to create stunning panoramas from multiple photos. Here's what you'll be doing:\n",
                "\n",
                "### What You'll Learn:\n",
                "1. **Projective Geometry and Homogeneous Coordinates:**  \n",
                "   We'll start by exploring how different transformations affect an image. You'll apply various transformations and visually experience how each one changes the image. Afterward, we'll tackle image rectification, which is how we can \"fix\" or align images using transformations, just like we did in Seminar 1.\n",
                "\n",
                "2. **Creating Panoramic Images:**  \n",
                "   Using the concept of homographies, you'll stitch together images captured from slightly different angles to form a seamless panoramic image. Think of it as turning a series of photos into one wide, beautiful view!\n",
                "\n",
                "### Deliverables:\n",
                "1. üíª **Complete the Code:**  \n",
                "   You'll be given some starter code that you'll need to complete. Once you‚Äôve filled in the missing parts, you'll submit your fully-executed Jupyter Notebook (.ipynb) with the code working as expected.\n",
                "\n",
                "2. üé• **Explain Your Work:**  \n",
                "   Record a video of yourself explaining the solutions to the questions in the lab, following the guidelines provided. It‚Äôs a great way to reinforce your understanding and demonstrate your progress!\n",
                "\n",
                "Let's get started and happy coding! üöÄ\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Introduction</strong>:\n",
                "  <ul>\n",
                "    <li>Provide an overall explanation of the lab. Describe the goals of the lab clearly.</li>\n",
                "    <li>State the problem you were trying to solve or explore in this lab.</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. Setting up the environment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 0.1 Environment Creation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Before starting the lab exercises, you must set up the working environment with any of the following options:\n",
                "- A) Local setup with a Conda environment. (Recommended)\n",
                "- B) Remote setup in Google Colab.\n",
                "- C) Local setup with Python (Fallback option)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "#### Option A: Local setup with a Conda environment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First of all, we recommend working on a conda environment. If conda is not installed, install it from https://www.anaconda.com/download/ .\n",
                "Then, run the following commands in the command line:\n",
                "\n",
                "```\n",
                "conda create --name 3d_vision python=3.11 -y\n",
                "conda activate 3d_vision\n",
                "```\n",
                "If you want, you can find further information about conda environments [here](https://medium.com/@viraj1604/comprehensive-guide-conda-virtual-environment-d70fafa7cf48).\n",
                "\n",
                "Then, install the dependencies using `pip`:\n",
                "```\n",
                "pip install jupyter ipykernel pillow numpy scipy matplotlib plotly opencv-python pandas\n",
                "python -m ipykernel install --user --name=3d_vision --display-name \"Python (3d_vision)\"\n",
                "```\n",
                "And make sure that the notebook is running with the recently created `3d_vision` environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os\n",
                "# env_name = os.environ['CONDA_DEFAULT_ENV'].split('/')[-1]\n",
                "# print(f\"Environment name: {env_name}\")\n",
                "# assert env_name == '3d_vision' # validating if you are running on 3d_vision conda environment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Option B: Remote setup in Google Colab"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "1. Open [Google Colab](https://colab.research.google.com) and upload this `.ipynb` file.\n",
                "2. Zip the folder with all the material for the current lab session.\n",
                "3. Upload the zipped folder anywhere in your Google Drive, share it for anyone with the link. You shoul obtain a link similar to `https://drive.google.com/file/d/14f1EP6UHr1Xk00X5n5Gjk8chBvCCX39/view?usp=share_link`.\n",
                "4. Copy the part the id from the link, i.e. the alphanumerical id between the last two slashes. In the previous example it would be `14f1EP6UHr1Xk00X5n5Gjk8chBvCCX39`\n",
                "5. Complete the cell below with the id and run it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# id = # Add your link id here\n",
                "# !mkdir Lab1\n",
                "# !gdown {id}\n",
                "# !unzip -q provided_files.zip -d Lab1\n",
                "# %cd Lab1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You may need to change directory again to be able to import utils\n",
                "# %cd provided_files"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have uploaded the directories and files required to run this Notebook, let's install the dependencies to be able to import the required libraries. Run the cell below to install all libraries. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %pip install pillow numpy scipy matplotlib plotly opencv-python pandas"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Option C: Local setup with Python (Fallback option)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you encountered any issues with the previous setups, you can install the necessary libraries directly into your Python environment using the following command:\n",
                "```\n",
                "pip install jupyter ipykernel pillow numpy scipy matplotlib plotly opencv-python pandas\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 0.2 Loading required libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import math\n",
                "import random\n",
                "\n",
                "import contextlib\n",
                "import numpy as np\n",
                "import plotly.express as px\n",
                "import scipy.io as sio\n",
                "from matplotlib import pyplot as plt\n",
                "from matplotlib.widgets import Slider\n",
                "from PIL import Image, ImageDraw\n",
                "from scipy.ndimage import map_coordinates\n",
                "import ipywidgets as widgets\n",
                "from ipywidgets import VBox\n",
                "from ipywidgets import interactive\n",
                "\n",
                "from IPython.display import Markdown, clear_output\n",
                "# import ipywidgets as widgets\n",
                "# from ipywidgets import interactive\n",
                "# from IPython.display import display, Markdown, clear_output\n",
                "\n",
                "\n",
                "from utils import apply_H_fixed_image_size, line_draw, DLT_homography"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Planar transformations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.1 Applying a homography to an image"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Below we provide the function `apply_H` which takes as input an image and a\n",
                "transformation (specified in a $3\\times 3$ matrix `H`) and applies the\n",
                "desired transformation to the input image. \n",
                "\n",
                "**Q1.1** Examine and understand the code. Complete the code following \n",
                "the directions in the function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_transformed_pixels_coords(I, H, shift=None):\n",
                "    \"\"\"Transforms pixel coordinates using a homography matrix.\n",
                "    \n",
                "    Args:\n",
                "        I (numpy.ndarray): Input image.\n",
                "        H (numpy.ndarray): Homography matrix.\n",
                "        shift (tuple, optional): Shift values for x and y coordinates. Defaults to None.\n",
                "    \n",
                "    Returns:\n",
                "        numpy.ndarray: Transformed pixel coordinates.\n",
                "    \"\"\"\n",
                "    # Get the height and width of the input image\n",
                "    I_height, I_width = I.shape[:2]\n",
                "    \n",
                "    # Generate indices for all pixel positions in the input image.\n",
                "    ys, xs = np.indices((I_height, I_width)).astype(\"float64\")\n",
                "\n",
                "    # Apply shift if provided\n",
                "    if shift is not None:\n",
                "        ys += shift[1]\n",
                "        xs += shift[0]\n",
                "        \n",
                "    # Transform from cartesian to projective coordinates: (x, y) -> (x, y, 1)\n",
                "    ws = np.ones((I_height, I_width))\n",
                "    coords = np.stack((xs, ys, ws), axis=2)\n",
                "    \n",
                "    # Apply the homography transformation to the coordinates: H * (x, y, 1)^T = (x', y', w')^T\n",
                "    coords_H = (H @ coords.reshape(-1, 3).T).T.reshape((I_height, I_width, 3))\n",
                "    \n",
                "    # Transform projective coordinates to cartesian: (x', y', w') -> (x'/w', y'/w')\n",
                "    cart_H = coords_H[:, :, :2] / coords_H[:, :, 2:]\n",
                "        \n",
                "    return cart_H\n",
                "\n",
                "def apply_H(I, H):\n",
                "    \"\"\"Apply homography transformation to an image.\n",
                "    \n",
                "    Args:\n",
                "        I (numpy.ndarray): Input image.\n",
                "        H (numpy.ndarray): Homography matrix.\n",
                "    \n",
                "    Returns:\n",
                "        numpy.ndarray: Transformed image.\n",
                "    \"\"\"\n",
                "    h, w = I.shape[:2]\n",
                "    \n",
                "    # corners\n",
                "    c1 = np.array([1, 1, 1])\n",
                "    c2 = np.array([w, 1, 1])\n",
                "    c3 = np.array([1, h, 1])\n",
                "    c4 = np.array([w, h, 1])\n",
                "    \n",
                "    # Compute the transformed homogeneous image corners according to H.\n",
                "    # Normalize the homogeneous coordinates so as the third coordinate is always 1.\n",
                "    # Call the transformed corners: Hc1, Hc2, Hc3, Hc4\n",
                "\n",
                "    # TODO: Transform corners according to H\n",
                "    # NOTE: * stands for element-wise multiplication while @ is typical mat/vec multiplication\n",
                "    Hc1 = H @ c1 # transformed corner c1\n",
                "    Hc2 = H @ c2 # transformed corner c2\n",
                "    Hc3 = H @ c3 # transformed corner c3\n",
                "    Hc4 = H @ c4 # transformed corner c4\n",
                "    \n",
                "    # TODO: Normalize homogeneous coordinates\n",
                "    # NOTE: When using /= the following error appears\n",
                "    # UFuncTypeError: Cannot cast ufunc 'divide' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'\n",
                "    # so I will just stick to more standard syntax\n",
                "    Hc1 = Hc1 / Hc1[2]\n",
                "    Hc2 = Hc2 / Hc2[2]\n",
                "    Hc3 = Hc3 / Hc3[2]\n",
                "    Hc4 = Hc4 / Hc4[2]\n",
                "    \n",
                "    # Compute extremal transformed corner coordinates\n",
                "    xmin = np.round(np.min([Hc1[0], Hc2[0], Hc3[0], Hc4[0]]))\n",
                "    xmax = np.round(np.max([Hc1[0], Hc2[0], Hc3[0], Hc4[0]]))\n",
                "    ymin = np.round(np.min([Hc1[1], Hc2[1], Hc3[1], Hc4[1]]))\n",
                "    ymax = np.round(np.max([Hc1[1], Hc2[1], Hc3[1], Hc4[1]]))\n",
                "    \n",
                "    # Calculate size of output image\n",
                "    size_x = math.ceil(xmax - xmin + 1)\n",
                "    size_y = math.ceil(ymax - ymin + 1)\n",
                "\n",
                "    # Create an empty output image\n",
                "    out = np.zeros((size_y, size_x, 3))\n",
                "\n",
                "    # Invert the homography matrix\n",
                "    H_inv = np.linalg.inv(H)\n",
                "    \n",
                "    # Define shift for map_coordinates function\n",
                "    shift = (xmin, ymin)\n",
                "    \n",
                "    # Get coordinates for interpolation\n",
                "    interpolation_coords = get_transformed_pixels_coords(out, H_inv, shift=shift)\n",
                "    interpolation_coords[:, :, [0, 1]] = interpolation_coords[:, :, [1, 0]]\n",
                "    interpolation_coords = np.swapaxes(np.swapaxes(interpolation_coords, 0, 2), 1, 2)\n",
                "\n",
                "    # Apply interpolation to each channel of the input image (R, G, and B)\n",
                "    out[:, :, 0] = map_coordinates(I[:, :, 0], interpolation_coords)\n",
                "    out[:, :, 1] = map_coordinates(I[:, :, 1], interpolation_coords)\n",
                "    out[:, :, 2] = map_coordinates(I[:, :, 2], interpolation_coords)\n",
                "  \n",
                "    return out.astype(\"uint8\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we are going to test the completed function `apply_H.m`\n",
                "with a hierarchy of 2D transformations. \n",
                "\n",
                "\n",
                "**Q1.2.** We want to apply a rotation of $30^o$ to the image. Which is the appropriate\n",
                "expression of matrix `H` below?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Write the expression for H\n",
                "rot = np.deg2rad(30)\n",
                "H = np.array([\n",
                "    [np.cos(rot), -np.sin(rot), 0],\n",
                "    [np.sin(rot),  np.cos(rot), 0],\n",
                "    [0,            0,           1]\n",
                "])\n",
                "\n",
                "# Convert matrix H to a LaTeX formatted string with actual values\n",
                "latex_str = r\"$H = \\begin{bmatrix} \" + \\\n",
                "    f\"{H[0, 0]:.4f} & {H[0, 1]:.4f} & {H[0, 2]:.4f} \\\\\\\\\" + \\\n",
                "    f\" {H[1, 0]:.4f} & {H[1, 1]:.4f} & {H[1, 2]:.4f} \\\\\\\\\" + \\\n",
                "    f\" {H[2, 0]:.4f} & {H[2, 1]:.4f} & {H[2, 2]:.4f}\" + \\\n",
                "    r\"\\end{bmatrix}$\"\n",
                "\n",
                "# Display it as Markdown\n",
                "display(Markdown(latex_str))\n",
                "\n",
                "img_path = \"./Data/mondrian.jpg\"\n",
                "I = Image.open(img_path)\n",
                "It = apply_H(np.array(I), H)\n",
                "\n",
                "plt.subplot(1,2,1)\n",
                "plt.title(\"Original image\")\n",
                "plt.imshow(I)\n",
                "plt.subplot(1,2,2)\n",
                "plt.title(\"Image rotated 30¬∞\")\n",
                "plt.imshow(It)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
                "    <strong>üé• Video Question:</strong> Regarding H, which kind of transformation is it? Why?\n",
                "  </ul>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Decomposing of a transformation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now apply the following transformation to the Mondrian image (complete the cell code below):\n",
                "$$ H = \\left(\\begin{array}{ccc}\n",
                "    2 & -1 & 1 \\\\\n",
                "    1 & 1 & 3 \\\\\n",
                "    0 & 0 & 1 \\\\\n",
                "   \\end{array} \\right)\n",
                "$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Write the matrix H\n",
                "H = np.array([\n",
                "    [2, -1, 1],\n",
                "    [1,  1, 3],\n",
                "    [0,  0, 1]\n",
                "])\n",
                "\n",
                "It = apply_H(np.array(I), H)\n",
                "\n",
                "plt.figure(figsize=(10,3))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.title(\"Original image\")\n",
                "plt.imshow(I)\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.title(\"Transformed image\")\n",
                "plt.imshow(It)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Video Question:</strong> Regarding this new homography, which kind of transformation is it? Why?\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "üíª **Code question**: Use the SVD decomposition (function `np.linalg.svd`) to express the\n",
                "transformation $H$ as a composition of two rotations, a pure anisotropic\n",
                "scaling and a pure translation (the rotations may also include a reflection)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Decompose H\n",
                "# construct T which only has translation info\n",
                "T = np.identity(3)\n",
                "T[0, 2] = H[0, 2]\n",
                "T[1, 2] = H[1, 2]\n",
                "\n",
                "# get the rotation and translation matrix A_p (A prime) from T and H\n",
                "A_p = np.linalg.inv(T) @ H\n",
                "\n",
                "# decompose A_p\n",
                "# first rotation is in U\n",
                "# scaling factors are the singular values\n",
                "# second rotation is Vh\n",
                "U, S, Vh = np.linalg.svd(A_p)\n",
                "\n",
                "# so that it is a 3x3 matrix\n",
                "S = np.diag(S)\n",
                "\n",
                "display(U)\n",
                "\n",
                "'''\n",
                "decompose s.t. first rotation, scaling, second rotation, translation\n",
                "\n",
                "first (T translation mat) is    I T\n",
                "            0 1\n",
                "\n",
                "second (rot + scaling mat) is   A 0\n",
                "            0 1\n",
                "svd the second\n",
                "\n",
                "first @ second = H\n",
                "'''"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Video Question:</strong><br>\n",
                "  <li> Which are the matrices corresponding to each one of the transformations?<br>\n",
                "  <li> Which is the appropriate order of composition?\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "To transform the image, we will apply the matrix transformations from right to left because:\n",
                "$$I'=HI=TU'S'V'^HI$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Apply these matrices to the image in the proper order, one by one,  with the function `apply_H` and verify that you get the same result than applying all the transformations at the same time using a single matrix. Provide in the cell code below all the commands used for this verification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualise the transformations step by step\n",
                "\n",
                "# TODO: Apply transformations step by step\n",
                "# NOTE: at first i computed it first with U for I1 and Vh for I3\n",
                "# but it is computed from right to left\n",
                "# e.g.: for each point i xi_prime = H @ x_i = T @ U @ S @ Vh @ x_i\n",
                "I1 = apply_H(np.array(I), Vh)\n",
                "I2 = apply_H(np.array(I1), S)\n",
                "I3 = apply_H(np.array(I2), U)\n",
                "I4 = apply_H(np.array(I3), T)\n",
                "\n",
                "# TODO: Add your list of transformed images step by step\n",
                "images = [I, I1, I2, I3, I4]\n",
                "\n",
                "plt.figure(figsize=(20,4))\n",
                "titles = [\"Original image\", \"First rotation\", \"Scaling\", \"Second rotation\", \"Translation (not visible)\"]\n",
                "for i, (image, title) in enumerate(zip(images, titles), start=1):\n",
                "    plt.subplot(1, 5, i)\n",
                "    plt.title(title)\n",
                "    plt.imshow(image)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Video Question:</strong> Do the sequence of transformations look right to you? Why?<br>\n",
                "  <em>(Don't worry if there is a translation difference in the last image)</em>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(12,4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.title(\"Image transformed in a single step\")\n",
                "plt.imshow(It)\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.title(\"Image transformed step by step\")\n",
                "# TODO: Add the final image after all transformations\n",
                "plt.imshow(I4)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Video Question</strong>: Do the original and transformed image look similar?<em>(Don't worry if there is a translation difference)</em></li>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Image Rectification"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We are going to test two image rectification methods\n",
                "1. Direct Linear Transformation\n",
                "2. Stratified Method"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Direct Linear Transformation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 2.1.1 Interactive method"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the first one we are going to remove the projective distortion from a perspective image of a plane (flat surface in the 3D world). The key idea is to select four coplanar points that should be mapped to a rectangle.\n",
                " "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_path = \"./Data/metro2.png\"\n",
                "I = Image.open(img_path)\n",
                "plt.imshow(I)\n",
                "plt.title(\"Original image\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "p1 = np.array([48.0, 537.0, 1])\n",
                "p2 = np.array([344.0, 480.0, 1])\n",
                "p3 = np.array([341.0, 329.0, 1])\n",
                "p4 = np.array([75.0, 265.0, 1])\n",
                "\n",
                "q1 = np.array([10.0, 200.0, 1])\n",
                "q2 = np.array([300.0, 200.0, 1])\n",
                "q3 = np.array([300.0, 10.0, 1])\n",
                "q4 = np.array([10.0, 10.0, 1])\n",
                "\n",
                "X = np.array([p1, p2, p3, p4])\n",
                "X = X.T\n",
                "Y = np.array([q1, q2, q3, q4])\n",
                "Y = Y.T\n",
                "\n",
                "I = Image.open(img_path)\n",
                "canv2 = ImageDraw.Draw(I)\n",
                "for i in range(4):\n",
                "    canv2.ellipse((X[0,i], X[1,i], X[0,i]+7, X[1,i]+7), fill = 'cyan', outline ='cyan')\n",
                "    \n",
                "\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Original image with selected points to become a rectangle\")\n",
                "plt.imshow(I)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The purpose now is to find the homography `H` that relates the two sets of\n",
                "points, encoded in the matrices `X` and `Y`, by using the normalised Direct Linear Transformation (DLT) algorithm.\n",
                "This algorithm is implemented in the provided function `DLT_homography` in the utils file. The\n",
                "matrix `H` is obtained by calling this function with the two matrices `X` and `Y` of\n",
                "homogeneous coordinates as inputs. Examine the function `DLT_homography` and identify the different steps of the algorithm studied in class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Apply the DLT homography\n",
                "H = DLT_homography(X, Y)\n",
                "It = apply_H(np.array(I), H)\n",
                "\n",
                "print(H)\n",
                "\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Image transformed with DLT\")\n",
                "plt.imshow(It)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Video Question</strong>: How does the DLT algorithm work in this example? Explain it briefly. <br>\n",
                "  <em>(You can show it in the DLT_homography function)</em>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 2.1.2 Rectifying a picture of a paper **(OPTIONAL)**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we are going rectify an image of a paper. Our goal is to apply a transformation to visualize the paper in a flattened position, as if we were viewing the original PDF file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_path = \"./Data/paper_picture_low_resolution.jpg\"\n",
                "I = Image.open(img_path)\n",
                "fig = px.imshow(I)\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Select the corners in the image above. For an accurate selection, you can zoom in by drawing a rectangle in the image, and then press autoscale from the options at the top right.\n",
                "\n",
                "In the cell below, what should be the values for `p1_paper`, `p2_paper`, `p3_paper`, and `p4_paper`? Fill them in, knowing that `pi_paper` will be transformed to the position `qi` for *i* ranging from 1 to 4."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find the coordinates of the points below\n",
                "# Notice the order of axes in numpy, 1st horizontal and 2nd vertical.\n",
                "# Being the origin (0,0) at the top-left of the image.\n",
                "# NOTE: order bugs me out\n",
                "p1_paper = np.array([1460, 803, 1])\n",
                "p2_paper = np.array([1316, 288, 1])\n",
                "p3_paper = np.array([403, 132, 1])\n",
                "p4_paper = np.array([27, 503, 1])\n",
                "\n",
                "q1 = np.array([0, 2830, 1]) # lower left corner\n",
                "q2 = np.array([2000, 2830, 1]) # lower right corner\n",
                "q3 = np.array([2000, 0, 1]) # upper right corner\n",
                "q4 = np.array([0, 0, 1]) # upper left corner\n",
                "\n",
                "X_paper = np.array([p1_paper, p2_paper, p3_paper, p4_paper])\n",
                "X_paper = X_paper.T\n",
                "Y = np.array([q1, q2, q3, q4])\n",
                "Y = Y.T\n",
                "\n",
                "# Displaying the image with chosen points\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.title(\"Picture of paper with selected corners in red\")\n",
                "for i, (x, y) in enumerate(zip(X_paper[0], X_paper[1]), start=1):\n",
                "    plt.scatter(x=x, y=y, c='r', s=20)\n",
                "    plt.text(x, y, f'$p_{i}$', fontsize=15, ha='right', va='bottom', color='red', weight='bold')\n",
                "plt.imshow(I)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the cell below, compute H using the DLT algorithm, and print the values of H and explain what type of transformation it represents."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Apply the DLT to find the homography\n",
                "H = DLT_homography(X_paper, Y)\n",
                "\n",
                "print(H)\n",
                "corners = [-1000, 6000, -2000, 5000]\n",
                "It = apply_H_fixed_image_size(np.array(I), H, corners)\n",
                "\n",
                "# Display the transformed picture\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.title(\"Transformed picture of the paper through DLT\")\n",
                "plt.imshow(It)\n",
                "for i, (x, y) in enumerate(zip(Y[0]-corners[0], Y[1]-corners[2]), start=1):\n",
                "    plt.scatter(x=x, y=y, c='r', s=20)\n",
                "    plt.text(x, y, f'$q_{i}$', fontsize=15, ha='right', va='bottom', color='red', weight='bold', style='oblique') \n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's crop the image to isolate the contents of the paper, excluding any surrounding background or unwanted elements."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate the minimum and maximum values for X and Y\n",
                "xmin, xmax = np.min(Y[0]) - corners[0], np.max(Y[0]) - corners[0]\n",
                "ymin, ymax = np.min(Y[1]) - corners[2], np.max(Y[1]) - corners[2]\n",
                "\n",
                "# Crop the the rectified image (notice the order of y and x)\n",
                "It_cropped = It[ymin:ymax, xmin:xmax]\n",
                "\n",
                "# Display the rectified portion of the image It\n",
                "plt.figure(figsize=(8, 14))\n",
                "plt.title(\"Rectified picture of the paper through DLT\")\n",
                "plt.imshow(It_cropped)\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Stratified method"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this part we will test a method for image rectification which works in two steps:\n",
                "1. Affine rectification (As in Seminar 1)\n",
                "2. Metric rectification (Optional)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 2.2.1-A) Affine rectification"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This first step uses the line at infinity. The rectification is based on transforming the identified \n",
                "image of the line at infinity to its canonical position of $\\ell_{\\infty} = (0, 0, 1)^T$. \n",
                "This is done by a projective transformation built from the coordinates of the imaged \n",
                "line at infinity, $\\ell = (\\ell_1, \\ell_2, \\ell_3)^T$, provided that $\\ell_3 \\neq 0$. \n",
                "The idea \n",
                "is illustrated in the following figure:\n",
                "\n",
                "![Title](./Data/Lab1-Stratified_method.drawio.png)\n",
                "\n",
                "<!-- ![Title](https://www.robots.ox.ac.uk/~vgg/hzbook/hzbook2/WebPage/pngfiles/projgeomfigs-affine_rect.png) -->\n",
                "\n",
                "We are going to compute $\\ell$, the imaged $\\ell_{\\infty}$, as the intersection \n",
                "of the two pair of lines formed by the four selected points in the previous exercise.\n",
                "For that, we are going to use the dual properties of homogeneous points and lines that define the line joining two points and the intersection of lines.\n",
                "\n",
                "The first approach to compute the line at infinity will be to use the four points manually selected previously. \n",
                "\n",
                "**Q2.1**\n",
                "Compute the vector `l1` representing the line that joins the two upper selected points.\n",
                "Compute the vector `l2` representing the line that joins the two lower selected points.\n",
                "Compute the vector `l3` representing the line that joins the two left selected points.\n",
                "Compute the vector `l4` representing the line that joins the two right selected points.\n",
                "\n",
                "SUGGESTION: You may use the numpy function `cross`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_path = \"./Data/metro2.png\"\n",
                "I = Image.open(img_path)\n",
                "\n",
                "# Display image with points\n",
                "plt.title(\"Original image with selected points\")\n",
                "plt.imshow(I)\n",
                "for i, (x, y) in enumerate(zip(X[0], X[1]), start=1):\n",
                "    plt.scatter(x=x, y=y, c='c', s=5)\n",
                "    plt.text(x, y, f'p{i}', fontsize=8, ha='right', va='bottom', color='cyan')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Compute the line vectors\n",
                "# NOTE: Should the order matter ???\n",
                "# NOTE: X is organized column-wise apparently\n",
                "l1 = np.cross(X[:, 3], X[:, 2])\n",
                "l2 = np.cross(X[:, 0], X[:, 1])\n",
                "l3 = np.cross(X[:, 0], X[:, 3])\n",
                "l4 = np.cross(X[:, 1], X[:, 2])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You may visualize the computed lines to check that your result is correct. \n",
                "Use the following commands to visualize the lines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function required to display lines\n",
                "def get_line_points(l, I):\n",
                "    x = np.arange(0, np.array(I).shape[1])\n",
                "    y = (-l[0] * x - l[2]) / l[1]\n",
                "    return x, y\n",
                "\n",
                "# Display image with lines and points\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Original image with computed lines and points\")\n",
                "for i, l in enumerate([l1, l2, l3, l4], start=1):\n",
                "    plt.plot(*get_line_points(l, I), label=f'$l_{i}$')\n",
                "for i, (x, y) in enumerate(zip(X[0], X[1]), start=1):\n",
                "    plt.scatter(x=x, y=y, c='c', s=5)\n",
                "    plt.text(x, y, f'$p_{i}$', fontsize=12, ha='right', va='bottom', color='cyan', weight='bold') \n",
                "plt.imshow(I)\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q2.2** \n",
                "Give the expression of a matrix `H_aff_rect` that maps the line \n",
                "$\\ell$ to the line $\\ell_{\\infty} = (0, 0, 1)^T$. Complete the code below in order to rectify the image by applying the computed homography. Compute also the transformed lines in order to visualize them as well."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Compute the vanishing point vectors and the line that passes through them\n",
                "v1 = np.cross(l1, l2)\n",
                "v2 = np.cross(l3, l4)\n",
                "l = np.cross(v1, v2)\n",
                "l = l / l[2] # Normalize the 3rd coordinate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Define the homography that affinely rectifies the image\n",
                "H_aff_rect = np.identity(3)\n",
                "H_aff_rect[2, :] = l # set last row to l\n",
                "\n",
                "print(\"H_aff_rect:\", H_aff_rect, sep=\"\\n\")\n",
                "\n",
                "# TODO: Compute the transformed image\n",
                "corners = [0, 1000, 0, 1000]\n",
                "I_aff_rect = apply_H_fixed_image_size(np.array(I), H_aff_rect, corners)\n",
                "\n",
                "# TODO: Compute the transformed lines\n",
                "H_aff_rect_mt = np.linalg.inv(H_aff_rect).T\n",
                "lr1 = H_aff_rect_mt @ l1\n",
                "lr2 = H_aff_rect_mt @ l2\n",
                "lr3 = H_aff_rect_mt @ l3\n",
                "lr4 = H_aff_rect_mt @ l4\n",
                "\n",
                "# TODO: Compute the transformed points\n",
                "q1 = H_aff_rect @ X[:, 0]\n",
                "q2 = H_aff_rect @ X[:, 1]\n",
                "q3 = H_aff_rect @ X[:, 2]\n",
                "q4 = H_aff_rect @ X[:, 3]\n",
                "\n",
                "\n",
                "# show the transformed lines and points in the transformed image\n",
                "I_tr = Image.fromarray(I_aff_rect, 'RGB')\n",
                "size = I_tr.size\n",
                "\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Rectified image with rectified lines and points\")\n",
                "\n",
                "# Display rectified lines\n",
                "for i, line in enumerate([lr1, lr2, lr3, lr4], start=1):\n",
                "    plt.plot(*get_line_points(line, I_tr), label=f'$lr_{i}$')\n",
                "\n",
                "# Display rectified points\n",
                "for i, (x, y, z) in enumerate([q1, q2, q3, q4], start=1):\n",
                "    x_norm, y_norm = x/z, y/z\n",
                "    plt.scatter(x=x_norm, y=y_norm, c='c', s=15)\n",
                "    plt.text(x_norm, y_norm, f'$q_{i}$', fontsize=12, ha='right', va='bottom', color='cyan', weight='bold') \n",
                "\n",
                "plt.legend()\n",
                "plt.imshow(I_tr)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Video Questions</strong>:\n",
                "  <ul style=\"margin-top: 5px; padding-left: 20px;\">\n",
                "    <li>What is the interpretation of the last row of the affine rectification matrix?</li>\n",
                "    <li>Which is the geometric relation of each pair of transformed lines before and after the transformation? (i.e., l‚ÇÅ with respect to l‚ÇÇ, lr‚ÇÅ with respect to lr‚ÇÇ, and so on with 3 and 4.)</li>\n",
                "    <li>What would be the result of the cross product of each pair of transformed lines (lr‚ÇÅ¬∑lr‚ÇÇ, and lr‚ÇÉ¬∑lr‚ÇÑ), and why?</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 2.2.1-B) Semi-automatic Affine Rectification **(OPTIONAL)**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, instead of manually selecting the projected parallel lines in the image we will use the vanishing points estimated by an automatic method for detecting vanishing points in urban scenes. It is based on the detection of line segments in the image and then identifying sets of converging segments and their intersection point by finding point alignments in a proper different space (more details, online demo and code available in the webage of the related publication http://www.ipol.im/pub/art/2017/148/).\n",
                "The result of the method applied to our image of interest is provided in the folder called 'vanishing_points'. The algorithm has detected four different vanishing points: their coordinates are given in matrix `vps` read in the code below. The lines corresponding to each of these points are depicted in the provided images in the folder 'vanishing_points'.\n",
                "\n",
                "Identify the proper pair of vanishing points to affinely rectify the image (justify your answer) and provide the code to obtain the rectified image with the new estimated vanishing points."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mat_content = sio.loadmat('./Data/vanishing_points/vps.mat')\n",
                "vps=mat_content['vps']\n",
                "print(vps)\n",
                "\n",
                "# complete the code that affinely rectifies the image and shows the transformed image ...\n",
                "'''\n",
                "Vanishing points 1 and 3 seem to fall inside the image which does not make sense\n",
                "VP 2 seems to be the vanishing point between lines 1 and 2\n",
                "VP 4 seems to be between lines 3 and 4\n",
                "'''\n",
                "l = np.cross(np.append(vps[:, 1], [1]), np.append(vps[:, 3], [1]))\n",
                "l = l / l[2]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# NOTE: this step does not change w.r.t. the last method\n",
                "\n",
                "H_aff_rect = np.identity(3)\n",
                "H_aff_rect[2, :] = l # set last row to l\n",
                "\n",
                "print(\"H_aff_rect:\", H_aff_rect, sep=\"\\n\")\n",
                "\n",
                "corners = [0, 1000, 0, 1000]\n",
                "I_aff_rect = apply_H_fixed_image_size(np.array(I), H_aff_rect, corners)\n",
                "\n",
                "# NOTE: even though the points and lines are not used in this section,\n",
                "# i think it is cool to see the same points and lines transformed to\n",
                "# see if there is any significant change\n",
                "H_aff_rect_mt = np.linalg.inv(H_aff_rect).T\n",
                "lr1 = H_aff_rect_mt @ l1\n",
                "lr2 = H_aff_rect_mt @ l2\n",
                "lr3 = H_aff_rect_mt @ l3\n",
                "lr4 = H_aff_rect_mt @ l4\n",
                "\n",
                "q1 = H_aff_rect @ X[:, 0]\n",
                "q2 = H_aff_rect @ X[:, 1]\n",
                "q3 = H_aff_rect @ X[:, 2]\n",
                "q4 = H_aff_rect @ X[:, 3]\n",
                "\n",
                "# show the transformed lines and points in the transformed image\n",
                "I_tr = Image.fromarray(I_aff_rect, 'RGB')\n",
                "size = I_tr.size\n",
                "\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Rectified image with rectified lines and points (Semi-automatic)\")\n",
                "\n",
                "# Display rectified lines\n",
                "for i, line in enumerate([lr1, lr2, lr3, lr4], start=1):\n",
                "    plt.plot(*get_line_points(line, I_tr), label=f'$lr_{i}$')\n",
                "\n",
                "# Display rectified points\n",
                "for i, (x, y, z) in enumerate([q1, q2, q3, q4], start=1):\n",
                "    x_norm, y_norm = x/z, y/z\n",
                "    plt.scatter(x=x_norm, y=y_norm, c='c', s=15)\n",
                "    plt.text(x_norm, y_norm, f'$q_{i}$', fontsize=12, ha='right', va='bottom', color='cyan', weight='bold') \n",
                "\n",
                "plt.legend()\n",
                "plt.imshow(I_tr)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### 2.2.2 Metric Rectification **(OPTIONAL)**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Metric rectification** typically comes after **affine rectification**, which corrects for the more basic linear distortions in the image (such as translation, rotation, and scaling). While affine rectification brings the image closer to the correct perspective, it does not necessarily preserve true distances or angles. Metric rectification goes a step further by adjusting the image to match the actual geometric properties of the scene.\n",
                "\n",
                "In other words, metric rectification allows us to adjust the image so that distances and angles in the image correspond to their true physical values, as if the image were taken from an orthographic or \"perfect\" perspective.\n",
                "\n",
                "In this example we would like the\n",
                "\n",
                "Play with interactive sliders to see how an affine transformation affects the rectified image. Can you find an affine transformation $H$ ($H_a^{-1}$ from the diagram above) that rotates and scales the image such that it appears as if the metro door is in front of the camera?\n",
                "\n",
                "$$H? \\quad \\Rightarrow \\left\\{\n",
                "\\begin{aligned}\n",
                "    l_{r1}, l_{r2} &\\text{ become horizontal lines} \\\\\n",
                "    l_{r3}, l_{r4} &\\text{ become vertical lines} \\\\\n",
                "    l_{r1}, l_{r2} &\\perp l_{r3}, l_{r4}\n",
                "\\end{aligned}\n",
                "\\right.$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Create the figure and axis ---\n",
                "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
                "plt.close(fig) # Keep this closed, display happens inside 'out'\n",
                "\n",
                "# --- Create an Output widget ---\n",
                "out = widgets.Output()\n",
                "\n",
                "# --- Initial affine homography matrix ---\n",
                "H_a = np.eye(3)\n",
                "\n",
                "# --- Function to apply transformation and update the Output widget ---\n",
                "def update_output(a11, a12, a21, a22, tx, ty):\n",
                "    global H_a\n",
                "    H_a = np.array([[a11, a12, tx], [a21, a22, ty], [0, 0, 1]])\n",
                "\n",
                "    rows, cols = I_aff_rect.shape[:2]\n",
                "    transformed_image = cv2.warpPerspective(\n",
                "        I_aff_rect, H_a, (cols, rows),\n",
                "        flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0)\n",
                "    )\n",
                "\n",
                "    ax.clear()\n",
                "    ax.imshow(transformed_image)\n",
                "    ax.set_title(f\"Affine Transformed Image\")\n",
                "    ax.axis('off')\n",
                "\n",
                "    latex_str = r\"$H_a = \\begin{bmatrix} \" + \\\n",
                "        f\"{H_a[0,0]:.4f} & {H_a[0,1]:.4f} & {H_a[0,2]:.4f} \\\\\\\\\" + \\\n",
                "        f\" {H_a[1,0]:.4f} & {H_a[1,1]:.4f} & {H_a[1,2]:.4f} \\\\\\\\\" + \\\n",
                "        f\" {H_a[2,0]:.4f} & {H_a[2,1]:.4f} & {H_a[2,2]:.4f}\" + \\\n",
                "        r\"\\end{bmatrix}$\"\n",
                "\n",
                "    # Update the Output widget's content\n",
                "    with out:\n",
                "        clear_output(wait=True)\n",
                "        display(\"Don't worry if the image and the matrix show multiple times. It's an error form our side.\")\n",
                "        display(fig)\n",
                "        display(Markdown(latex_str))\n",
                "\n",
                "# --- Create the sliders individually ---\n",
                "slider_opts = {'continuous_update': False, 'readout_format': '.2f'}\n",
                "a11_slider = widgets.FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.01, description='a11:', **slider_opts)\n",
                "a12_slider = widgets.FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.01, description='a12:', **slider_opts)\n",
                "a21_slider = widgets.FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.01, description='a21:', **slider_opts)\n",
                "a22_slider = widgets.FloatSlider(value=1.0, min=-3.0, max=3.0, step=0.01, description='a22:', **slider_opts)\n",
                "tx_slider = widgets.FloatSlider(value=0, min=-200, max=200, step=1, description='Translate X:', continuous_update=False, readout_format='.0f')\n",
                "ty_slider = widgets.FloatSlider(value=0, min=-200, max=200, step=1, description='Translate Y:', continuous_update=False, readout_format='.0f')\n",
                "\n",
                "# --- Use interactive_output to link sliders to the function ---\n",
                "interactive_updater = widgets.interactive_output(update_output, {\n",
                "    'a11': a11_slider, 'a12': a12_slider, 'a21': a21_slider,\n",
                "    'a22': a22_slider, 'tx': tx_slider, 'ty': ty_slider\n",
                "})\n",
                "\n",
                "# --- Arrange the UI: Sliders and the Output widget ---\n",
                "controls = VBox([a11_slider, a12_slider, a21_slider, a22_slider, tx_slider, ty_slider])\n",
                "# The 'out' widget is where the function sends its display calls.\n",
                "ui = VBox([controls, out])\n",
                "\n",
                "# --- Display the UI ---\n",
                "display(ui)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "H_a = np.array([\n",
                "    [1.71, 0.23, 0],\n",
                "    [-0.69, 1.24, 0],\n",
                "    [0, 0, 1]\n",
                "])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We know that $H = H_a * H_p$, so we can do the following by pre-computing H and transforming the original image, or we can concatenate the inverse transformation H_a to the previously done transformation (H_aff_rect). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"H_a:\", H_a, sep=\"\\n\")\n",
                "\n",
                "corners = [0, 1500, 0, 800]\n",
                "I_rect = apply_H_fixed_image_size(I_aff_rect, H_a, corners)\n",
                "\n",
                "# NOTE: even though the points and lines are not used in this section,\n",
                "# i think it is cool to see the same points and lines transformed to\n",
                "# see if there is any significant change\n",
                "H_a_mt = np.linalg.inv(H_a).T\n",
                "lr1 = H_a_mt @ H_aff_rect_mt @ l1\n",
                "lr2 = H_a_mt @ H_aff_rect_mt @ l2\n",
                "lr3 = H_a_mt @ H_aff_rect_mt @ l3\n",
                "lr4 = H_a_mt @ H_aff_rect_mt @ l4\n",
                "\n",
                "q1 = H_a @ H_aff_rect @ X[:, 0]\n",
                "q2 = H_a @ H_aff_rect @ X[:, 1]\n",
                "q3 = H_a @ H_aff_rect @ X[:, 2]\n",
                "q4 = H_a @ H_aff_rect @ X[:, 3]\n",
                "\n",
                "# show the transformed lines and points in the transformed image\n",
                "I_tr = Image.fromarray(I_rect, 'RGB')\n",
                "size = I_tr.size\n",
                "\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Rectified image with rectified lines and points\")\n",
                "\n",
                "# Display rectified lines\n",
                "for i, line in enumerate([lr1, lr2, lr3, lr4], start=1):\n",
                "    plt.plot(*get_line_points(line, I_tr), label=f'$lr_{i}$')\n",
                "\n",
                "# Display rectified points\n",
                "for i, (x, y, z) in enumerate([q1, q2, q3, q4], start=1):\n",
                "    x_norm, y_norm = x/z, y/z\n",
                "    plt.scatter(x=x_norm, y=y_norm, c='c', s=15)\n",
                "    plt.text(x_norm, y_norm, f'$q_{i}$', fontsize=12, ha='right', va='bottom', color='cyan', weight='bold') \n",
                "\n",
                "plt.legend()\n",
                "plt.imshow(I_tr)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Image mosaics"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The images that form the mosaic are related by homographies that we will need to compute.  The process will have the following steps:\n",
                "\n",
                "- The first step will be to compute keypoints in the images that we can use to find correspondences.  We will use SIFT or ORB keypoints and features.\n",
                "- Next we will compute correspondences between sets of keypoint features. We will use existing code for these first two steps.\n",
                "- From the correspondences we will compute the homography that maps one image into the another. Since some correspondences may be erroneous, the computation will have to be robust to outliers. We will use the \"RANdom SAmple Consensus\" (RANSAC) method that you will have to complete.\n",
                "- Finally, with the homographies computed, we will map all the images into a common canvas by using a variant of the  `apply_H` function from the last assignment.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.0 Visualize the original images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reading images using OpenCV\n",
                "img1c = cv2.imread('Data/llanes_a.jpg', cv2.IMREAD_COLOR)\n",
                "img2c = cv2.imread('Data/llanes_b.jpg', cv2.IMREAD_COLOR)\n",
                "img3c = cv2.imread('Data/llanes_c.jpg', cv2.IMREAD_COLOR)\n",
                "\n",
                "# Converting color space from BGR to RGB using OpenCV\n",
                "img1c = cv2.cvtColor(img1c, cv2.COLOR_BGR2RGB)\n",
                "img2c = cv2.cvtColor(img2c, cv2.COLOR_BGR2RGB)\n",
                "img3c = cv2.cvtColor(img3c, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "# Creating a Matplotlib figure for displaying images\n",
                "plt.figure(figsize=(20,5))\n",
                "\n",
                "# Iterating over the images and plotting them\n",
                "for i, image in enumerate([img1c, img2c, img3c], start=1):\n",
                "    plt.subplot(1, 3, i)\n",
                "    plt.title(f\"Image {i}\")\n",
                "    plt.imshow(image)\n",
                "\n",
                "# Displaying the Matplotlib figure with images\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Compute image correspondences"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The first step is to read the images and to compute their keypoints. The images are RGB. We have to convert them to gray scale with values in order to compute the keypoints on them. Then, we compute the keypoints and descriptors of every image. For the parts where the content of the two images coincide, you can visually check that many of the detected points are detected in both images. We want to find these _correspondences_."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reading images using OpenCV in gray scale\n",
                "img1 = cv2.imread('Data/llanes_a.jpg', cv2.IMREAD_GRAYSCALE)\n",
                "img2 = cv2.imread('Data/llanes_b.jpg', cv2.IMREAD_GRAYSCALE)\n",
                "img3 = cv2.imread('Data/llanes_c.jpg', cv2.IMREAD_GRAYSCALE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To match the keypoints between two images, we need to assign to each keypoint in the first image the one that has the most similar descriptor in the second image. \n",
                "\n",
                "Execute the following code to find image correspondences using SIFT [1].\n",
                "\n",
                "[1] David Lowe. Object recognition from local scale-invariant features. ICCV, 1150-1157, 1999.\n",
                "\n",
                "<!-- [3] Herbert Bay, Tinne Tuytelaars, Luc Van Gool. Surf: Speeded up robust features. ECCV, 404-417, 2006. -->"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initiating SIFT detector\n",
                "sift = cv2.SIFT_create(3000)\n",
                "\n",
                "# Finding the keypoints and descriptors\n",
                "kp1, des1 = sift.detectAndCompute(img1, None)\n",
                "kp2, des2 = sift.detectAndCompute(img2, None)\n",
                "\n",
                "# Drawing only keypoints location, not size and orientation\n",
                "img1b = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
                "img2b = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
                "\n",
                "# Showing keypoints in images\n",
                "plt.figure(figsize=(13,5))\n",
                "plt.suptitle(\"Keypoints extracted using SIFT\")\n",
                "for i, image in enumerate([img1b, img2b], start=1):\n",
                "    plt.subplot(1, 2, i)\n",
                "    plt.title(f\"Keypoints for Image {i}\")\n",
                "    plt.imshow(image)\n",
                "plt.show()\n",
                "\n",
                "# Keypoint matching\n",
                "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
                "matches_12 = bf.match(des1, des2)\n",
                "\n",
                "# Show matches\n",
                "img_12 = cv2.drawMatches(img1, kp1, img2, kp2, matches_12, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Matching keypoints from images 1 and 2 with SIFT\")\n",
                "plt.imshow(img_12)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q3.1**\n",
                "Compute and visualize the keypoints and matchings between image 2 and 3. Write the commands you used for that."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Correspondences between image 2 and 3\n",
                "kp3, des3 = sift.detectAndCompute(img3, None)\n",
                "\n",
                "img2b = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
                "img3b = cv2.drawKeypoints(img3, kp3, None, color=(0,255,0), flags=0)\n",
                "\n",
                "# Showing keypoints in images\n",
                "plt.figure(figsize=(13,5))\n",
                "plt.suptitle(\"Keypoints extracted using SIFT\")\n",
                "for i, image in enumerate([img1b, img2b], start=1):\n",
                "    plt.subplot(1, 2, i)\n",
                "    plt.title(f\"Keypoints for Image {i}\")\n",
                "    plt.imshow(image)\n",
                "plt.show()\n",
                "\n",
                "# Keypoint matching\n",
                "matches_23 = bf.match(des2, des3)\n",
                "\n",
                "# Show matches\n",
                "img_23 = cv2.drawMatches(img2, kp2, img3, kp3, matches_23, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Matching keypoints from images 1 and 2 with SIFT\")\n",
                "plt.imshow(img_23)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(230, 242, 255, 0) 255, 255);\">\n",
                "  <strong>üé• Video Questions</strong>:\n",
                "  <ul style=\"margin-top: 5px; padding-left: 20px;\">\n",
                "    <li>Explain the result after applying keypoint matching.</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Compute the homography (robust DLT algorithm) between image pairs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We want now to compute the homography that relates each pair of images. From  the last assignment, we have a function called `DLT_homography` that computes a homography given a set of correspondences. Unfortunately, this only works when all of the correspondences are correct, which is not the case in most practical applictions as the current one. This time, we will need to use the RANSAC method in order to find the correct correspondences and discard the others.\n",
                "\n",
                "For that we use the functions `Ransac_DLT_homography` and `find_homography_inliers` that you have \n",
                "to complete below. Complete the `Ransac_DLT_homography` function by answering the following questions:\n",
                "\n",
                "**Q3.2** Set the number of samples to choose randomly and that will define the model to test in each trial. (second input parameter of function `random.sample`).\n",
                "\n",
                "**Q3.3** Complete the function `Inliers` by computing the geometric error of the correspondences given the homography."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_homography_inliers(H, points1, points2, th):\n",
                "    \"\"\"Finds the inliers between two sets of points using a homography matrix.\n",
                "    \n",
                "    Args:\n",
                "        H (numpy.ndarray): Homography matrix.\n",
                "        points1 (numpy.ndarray): Points from the first image.\n",
                "        points2 (numpy.ndarray): Points from the second image.\n",
                "        th (float): Threshold for considering a point as an inlier.\n",
                "    \n",
                "    Returns:\n",
                "        numpy.ndarray: Indices of the inliers in the consensus set.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        H_inv = np.linalg.inv(H)\n",
                "    except np.linalg.LinAlgError:\n",
                "        print(\"Matrix H is not invertible.\")\n",
                "        return np.empty(0)  # Return empty array if matrix is not invertible\n",
                "    \n",
                "    # TODO: Complete this code\n",
                "    errors_fwd = np.linalg.norm(H @ points1 - points2, axis=0)\n",
                "    errors_bwd = np.linalg.norm(H_inv @ points2 - points1, axis=0)\n",
                "\n",
                "    errors = (errors_fwd + errors_bwd) / 2\n",
                "\n",
                "    inliers = errors < th\n",
                "\n",
                "    inliers_indices = np.where(inliers)[0]\n",
                "\n",
                "    return inliers_indices"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Complete the function `Ransac_DLT_homography()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def Ransac_DLT_homography(points1, points2, th, N):\n",
                "    \n",
                "    Ncoords, Npts = points1.shape\n",
                "    \n",
                "    it = 0\n",
                "    best_inliers = np.empty(1)\n",
                "    # TODO: Fill the value of s\n",
                "    s = 4 # minimum correspondences\n",
                "\n",
                "    while N > it:\n",
                "        # Sampling indices and finding inliers\n",
                "        indices = random.sample(range(Npts), s)\n",
                "        H = DLT_homography(points1[:,indices], points2[:,indices])\n",
                "        inliers = find_homography_inliers(H, points1, points2, th)\n",
                "        \n",
                "        # test if it is the best model so far\n",
                "        if len(inliers) > len(best_inliers):\n",
                "            best_inliers = inliers\n",
                "        \n",
                "        it += 1\n",
                "    \n",
                "    print(f\"Number of iterations: {it}, num inliers: {len(best_inliers)}\")\n",
                "    # compute H from all the inliers\n",
                "    H = DLT_homography(points1[:,best_inliers], points2[:,best_inliers])\n",
                "    inliers_recomputed = find_homography_inliers(H, points1, points2, th)\n",
                "    print(f\"Recomputed inliers: {len(inliers_recomputed)}\")\n",
                "    \n",
                "    return H, best_inliers\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Video Questions</strong>:\n",
                "  <ul style=\"margin-top: 5px; padding-left: 20px;\">\n",
                "    <li>Briefly explain how <b>RANSAC DLT Homography</b> works.</li>\n",
                "    <li>What is <code>s</code>? Why have you selected that value?</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following code allows to robustly estimate the homography that relates images 1 and 2. Examine the code and answer the questions below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Homography between images 1 and 2\n",
                "points1_H12 = []\n",
                "points2_H12 = []\n",
                "for m in matches_12:\n",
                "    points1_H12.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1])\n",
                "    points2_H12.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1])\n",
                "    \n",
                "points1_H12 = np.asarray(points1_H12)\n",
                "points1_H12 = points1_H12.T\n",
                "points2_H12 = np.asarray(points2_H12)\n",
                "points2_H12 = points2_H12.T\n",
                "\n",
                "# TODO: Find a value for th\n",
                "th = 1 # max. allowed error in pixels\n",
                "\n",
                "H_12, indices_inlier_matches_12 = Ransac_DLT_homography(points1_H12, points2_H12, th, 1000)\n",
                "inlier_matches_12 = [matches_12[i] for i in indices_inlier_matches_12]\n",
                "\n",
                "img_12 = cv2.drawMatches(img1, kp1, img2, kp2, inlier_matches_12, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
                "\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Inlier correspondences between image 1 and image 2\")\n",
                "plt.imshow(img_12)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color:rgba(255, 255, 255, 0);\">\n",
                "  <strong>üé• Video Questions</strong>:\n",
                "  <ul style=\"margin-top: 5px; padding-left: 20px;\">\n",
                "    <li>What is <code>th</code>? Justify the chosen value.</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q3.5** Create a new function `Ransac_DLT_homography_adaptive_loop` which is based on the function `Ransac_DLT_homography` and automatically adapts the number of trials to ensure we pick, with a probability $p=0.99$ an initial data set with no outliers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def Ransac_DLT_homography_adaptative_loop(points1, points2, th, N):\n",
                "# TODO: Fill this function\n",
                "...\n",
                "\n",
                "    return H, best_inliers\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q3.6** Compare experimentally the two versions of the RANSAC algorithm (`Ransac_DLT_homography` and `Ransac_DLT_homography_adaptive_loop`) in terms of the number of iterations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Ransac_DLT_homography\")\n",
                "H_12, indices_inlier_matches_12 = Ransac_DLT_homography(points1_H12, points2_H12, th, 1000)\n",
                "\n",
                "print(\"\\n\\nRansac_DLT_homography_adaptative_loop\")\n",
                "H_12, indices_inlier_matches_12 = Ransac_DLT_homography_adaptative_loop(points1_H12, points2_H12, th, 1000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's compare how long do these functions take."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Suppress all output\n",
                "with open(os.devnull, 'w') as fnull:\n",
                "    with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
                "        time_fixed_loop = %timeit -r 3 -q -o H_12, indices_inlier_matches_12 = Ransac_DLT_homography(points1_H12, points2_H12, th, 1000)\n",
                "        time_adapt_loop = %timeit -r 3 -q -o H_12, indices_inlier_matches_12 = Ransac_DLT_homography_adaptative_loop(points1_H12, points2_H12, th, 1000)\n",
                "print(f\"Ransac_DLT_homography \\t\\t\\t{time_fixed_loop}\")\n",
                "print(f\"Ransac_DLT_homography_adaptative_loop  \\t{time_adapt_loop}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: #e6f2ff;\">\n",
                "  <strong>üé• Video Questions</strong>:\n",
                "  <ul style=\"margin-top: 5px; padding-left: 20px;\">\n",
                "    <li>Given the comparisons of above, which algorithm do you prefer? Briefly explain the difference between the algorithms.</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**Q3.7** Compute the homography that relates images 2 and 3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "points2_H23 = []\n",
                "points3_H23 = []\n",
                "\n",
                "# TODO: Find the homography between images 2 and 3\n",
                "...\n",
                "\n",
                "\n",
                "# Plot inlier correspondences\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Inlier correspondences between image 2 and image 3\")\n",
                "plt.imshow(img_23)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Build the mosaic"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "At this point we have all the ingredients to build the image mosaic. For transforming an image with a specified homography we use a modification of the function `apply_H` used at the beginning of this session. The modified function `apply_H_fixed_image_size` transforms the input image according to the input homography and writes it in an output image of size corresponding to the input vector of desired corner coordinates.\n",
                "\n",
                "Examine and complete the code below when necessary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mosaic corners\n",
                "MOSAIC_CORNERS = [-400, 1200, -100, 650]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Building the mosaic for images 1-2\n",
                "img1c_w = apply_H_fixed_image_size(img1c, H_12, MOSAIC_CORNERS)\n",
                "img2c_w = apply_H_fixed_image_size(img2c, np.identity(3), MOSAIC_CORNERS)\n",
                "img_mosaic_12 = np.maximum(img1c_w, img2c_w)\n",
                "plt.figure(figsize=(18.5, 10.5))\n",
                "plt.title(\"Mosaic with images 1 and 2\")\n",
                "plt.imshow(img_mosaic_12)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: #e6f2ff;\">\n",
                "  <strong>üé• Video Questions</strong>: Regarding the mosaic with images 1 & 2:\n",
                "  <ul style=\"margin-top: 5px; padding-left: 20px;\">\n",
                "    <li>Which homography has been applied to image 1? And to image 2? Why?</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Build the mosaic for images 2-3\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Build the mosaic for images 1-2-3\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: #e6f2ff;\">\n",
                "  <strong>üé• Video Questions</strong>: Regarding the mosaic with images 1, 2 & 3:\n",
                "  <ul style=\"margin-top: 5px; padding-left: 20px;\">\n",
                "    <li>Does it look right to you?</li>\n",
                "    <li>Which homography is applied to image 1?</li>\n",
                "    <li>Which homography is applied to image 2?</li>\n",
                "    <li>Which homography do you apply to image 3? Why?</li>\n",
                "  </ul>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Build your own mosaic **(OPTIONAL)**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Build a mosaic with your own set of images and explain why it works well or it doesn't."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. References"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Add here the material you used to complete this Lab. Cite and describe the usage of AI tools if any was used according to the Guidelines for AI tools."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TODO: Complete"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: #e6f2ff;\">\n",
                "  <strong>üé• Video Questions</strong>: Briefly mention the references.\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"border: 2px solid #007acc; border-radius: 10px; padding: 10px; background-color: #e6f2ff;\">\n",
                "  <strong>üé• Self-Assessment and Conclusions</strong>:\n",
                "  <ul>\n",
                "  <li><b>Which parts of the notebook did you succeed in? </b><br>\n",
                "  <em>Describe the sections where you felt confident, and explain why you think they were successful.</em></li>\n",
                "  <li><b>Which parts of the notebook did you fail to solve? </b><br>\n",
                "  <em>Be honest about the areas where you faced difficulties. What challenges or issues did you encounter that you couldn‚Äôt resolve? How would you approach these issues in the future?</em></li>\n",
                "  </ul>\n",
                "  Is there anything else that you would like to comment?\n",
                "</div>\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
